{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "\n",
    "import string\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = False\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 2428 -> polkadots\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path+'/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path+'/word2Id.npy'))\n",
    "id2word_dict =  dict(np.load(dictionary_path+'/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s'%('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s'%('2428', id2word_dict['2428']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s'%(word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    tokens = []\n",
    "    tokens.extend(nltk.tokenize.word_tokenize(prep_line.lower()))\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [word2Id_dict[tokens[k]] if tokens[k] in word2Id_dict else word2Id_dict['<RARE>'] for k in range(len(tokens))]\n",
    "    \n",
    "    return line\n",
    "\n",
    "#nltk.download('punkt')\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path+'/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data'%(n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_DEPTH = 3\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    imagefile = tf.read_file(image_path)\n",
    "    image = tf.image.decode_image(imagefile, channels=3)\n",
    "    float_img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    float_img.set_shape([None, None, 3])\n",
    "    image = tf.image.resize_images(float_img, size = [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "    \n",
    "    return image, caption\n",
    "\n",
    "def data_iterator(filenames, batch_size, data_generator):\n",
    "    # Load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i])) \n",
    "    caption = np.asarray(caption)\n",
    "    #print(df['ImagePath'].values.shape)\n",
    "    image_path = 'dataset/'+df['ImagePath'].values\n",
    "    \n",
    "    # Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == df['ImagePath'].values.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH_SIZE = 64\n",
    "iterator_train, types, shapes = data_iterator(data_path+'/text2ImgData.pkl', BATCH_SIZE, training_data_generator)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    image, text = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text (a list of id)\n",
    "    output: hidden representation of input text in dimention of TEXT_DIM\n",
    "    \"\"\"\n",
    "    def __init__(self, text, hparas, training_phase=True, reuse=False, return_embed=False):\n",
    "        self.text = text\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('rnnftxt', reuse=self.reuse):\n",
    "            # Word embedding\n",
    "            word_embed_matrix = tf.get_variable('rnn/wordembed', \n",
    "                                                shape=(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM']),\n",
    "                                                initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                                                dtype=tf.float32)\n",
    "            embedded_word_ids = tf.nn.embedding_lookup(word_embed_matrix, self.text)\n",
    "#             network = EmbeddingInputlayer(\n",
    "#                                  inputs = self.text,\n",
    "#                                  vocabulary_size = self.hparas['VOCAB_SIZE'],\n",
    "#                                  embedding_size = self.hparas['EMBED_DIM'],\n",
    "#                                  E_init = tf.random_normal_initializer(stddev=0.02),\n",
    "#                                  name = 'rnn/wordembed')\n",
    "            # RNN encoder\n",
    "            LSTMCell = tf.nn.rnn_cell.LSTMCell(self.hparas['TEXT_DIM'], reuse=self.reuse)\n",
    "            LSTMCell = tf.contrib.rnn.AttentionCellWrapper(LSTMCell, attn_length=40, state_is_tuple=True)\n",
    "            keep_prob = tf.cond(tf.constant(self.train, dtype=tf.bool), lambda:tf.constant(self.hparas['KEEP_PROB']), lambda:tf.constant(1.0))\n",
    "            LSTMCell = tf.nn.rnn_cell.DropoutWrapper(LSTMCell, input_keep_prob = keep_prob, output_keep_prob = keep_prob)\n",
    "            initial_state = LSTMCell.zero_state(self.hparas['BATCH_SIZE'], dtype=tf.float32)\n",
    "            rnn_net = tf.nn.dynamic_rnn(cell=LSTMCell, \n",
    "                                        inputs=embedded_word_ids, \n",
    "                                        initial_state=initial_state, \n",
    "                                        dtype=np.float32, time_major=False,\n",
    "                                        scope='rnn/dynamic')\n",
    "            \n",
    "            \n",
    "#             network = DynamicRNNLayer(network,\n",
    "#                      cell_fn = LSTMCell,\n",
    "#                      cell_init_args = {'state_is_tuple' : True, 'reuse': self.reuse},  # for TF1.1, TF1.2 dont need to set reuse\n",
    "#                      n_hidden = self.hparas['RNN_HIDDEN_SIZE'],\n",
    "#                      dropout = (self.hparas['KEEP_PROB'] if self.train else None),\n",
    "#                      initializer = tf.random_normal_initializer(stddev=0.02),\n",
    "#                      sequence_length = tl.layers.retrieve_seq_length_op2(self.text),\n",
    "#                      return_last = True,\n",
    "#                      name = 'rnn/dynamic')\n",
    "            self.rnn_net = rnn_net\n",
    "            self.outputs = rnn_net[0][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder:\n",
    "    def __init__(self, image, hparas, training_phase=True, reuse=False, return_embed=False):\n",
    "        self.image = image\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        kernal_init = tf.random_normal_initializer(stddev=0.02)\n",
    "        gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "        df_dim = 64\n",
    "        with tf.variable_scope('cnnftxt', reuse=self.reuse):\n",
    "            net0 = tf.layers.conv2d(self.image, df_dim, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'cnn/conv2d1')\n",
    "            net1 = tf.layers.conv2d(net0, df_dim*2, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'cnn/conv2d2')\n",
    "            #net1 = tf.layers.batch_normalization(net1, training = self.train, gamma_initializer = gamma_init, name='cnn/batch_norm1')\n",
    "            net1 = tf.contrib.layers.batch_norm(net1, decay=0.9, center=True, scale=True, is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net1 = tf.nn.leaky_relu(net1, 0.2)\n",
    "            net2 = tf.layers.conv2d(net1, df_dim*4, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'cnn/conv2d3')\n",
    "            #net2 = tf.layers.batch_normalization(net2, training = self.train, gamma_initializer = gamma_init, name='cnn/batch_norm2')\n",
    "            net2 = tf.contrib.layers.batch_norm(net2, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net2 = tf.nn.leaky_relu(net2, 0.2)\n",
    "            net3 = tf.layers.conv2d(net2, df_dim*8, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'cnn/conv2d4')\n",
    "            #net3 = tf.layers.batch_normalization(net3, training = self.train, gamma_initializer = gamma_init, name='cnn/batch_norm3')\n",
    "            net3 = tf.contrib.layers.batch_norm(net3, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            \n",
    "            net4 = tf.contrib.layers.flatten(net3)\n",
    "            net4 = tf.layers.dense(net4, self.hparas['TEXT_DIM'], name='cnn/dense', reuse=self.reuse)\n",
    "            \n",
    "            self.outputs = net4\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, noise_z, text, training_phase, hparas, reuse):\n",
    "        self.z = noise_z\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.gf_dim = 128\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('generator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM'], name='generator0/text_input', reuse=self.reuse)\n",
    "            text_input = tf.nn.leaky_relu(text_input, 0.2)\n",
    "            z_text_concat = tf.concat([self.z, text_input], axis=1, name='generator0/z_text_concat')\n",
    "            \n",
    "            kernal_init = tf.truncated_normal_initializer(stddev=0.02)\n",
    "            gamma_init=tf.random_normal_initializer(1., 0.02)\n",
    "            beta_init=tf.zeros_initializer()\n",
    "            moving_mean_init=tf.zeros_initializer()\n",
    "            moving_variance_init=tf.ones_initializer()\n",
    "            param_initializers = {'beta': beta_init,\n",
    "                                 'gamma': gamma_init,\n",
    "                                 'moving_mean': moving_mean_init,\n",
    "                                 'moving_variance': moving_variance_init}\n",
    "            gf_dim = 128\n",
    "            \n",
    "            g_net0 = tf.layers.dense(z_text_concat, gf_dim*8*4*4, name='generator0/g_net', reuse=self.reuse)\n",
    "            #g_net0 = tf.layers.batch_normalization(g_net0, training = self.train, gamma_initializer = gamma_init, name='generator0/batch_norm')\n",
    "            g_net0 = tf.contrib.layers.batch_norm(g_net0, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = None)\n",
    "            g_net0 = tf.reshape(g_net0, [-1, 4, 4, gf_dim*8], name='generator0/g_net_reshape')\n",
    "            \n",
    "            ######1\n",
    "            g_net = tf.layers.conv2d(g_net0, gf_dim*2, (1, 1), (1, 1), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                name = 'generator1/res/conv2d1')\n",
    "            #g_net = tf.layers.batch_normalization(g_net, training = self.train, gamma_initializer = gamma_init, name='generator1/batch_norm1')\n",
    "            g_net = tf.contrib.layers.batch_norm(g_net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "            g_net = tf.layers.conv2d(g_net, gf_dim*2, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generator1/res/conv2d2')\n",
    "            #g_net = tf.layers.batch_normalization(g_net, training = self.train, gamma_initializer = gamma_init, name='generator1/batch_norm2')\n",
    "            g_net = tf.contrib.layers.batch_norm(g_net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "            g_net = tf.layers.conv2d(g_net, gf_dim*8, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generator1/res/conv2d3')\n",
    "            #g_net = tf.layers.batch_normalization(g_net, training = self.train, gamma_initializer = gamma_init, name='generator1/batch_norm3')\n",
    "            g_net = tf.contrib.layers.batch_norm(g_net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = None)\n",
    "            g_net1 = tf.add(g_net, g_net0)\n",
    "            g_net1 = tf.nn.relu(g_net1)\n",
    "            \n",
    "            ######2\n",
    "            g_net2 = tf.image.resize_images(g_net1, [8, 8], method = 1, align_corners = False)\n",
    "            g_net2 = tf.layers.conv2d(g_net2, gf_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generator2/conv2d1')\n",
    "            #g_net2 = tf.layers.batch_normalization(g_net2, training = self.train, gamma_initializer = gamma_init, name='generator2/batch_norm1')\n",
    "            g_net2 = tf.contrib.layers.batch_norm(g_net2, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "            \n",
    "            ######3\n",
    "            g_net = tf.layers.conv2d(g_net2, gf_dim, (1, 1), (1, 1), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                name = 'generator3/res/conv2d1')\n",
    "            #g_net = tf.layers.batch_normalization(g_net, training = self.train, gamma_initializer = gamma_init, name='generator3/batch_norm1')\n",
    "            g_net = tf.contrib.layers.batch_norm(g_net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "            g_net = tf.layers.conv2d(g_net, gf_dim, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generator3/res/conv2d2')\n",
    "            #g_net = tf.layers.batch_normalization(g_net, training = self.train, gamma_initializer = gamma_init, name='generator3/batch_norm2')\n",
    "            g_net = tf.contrib.layers.batch_norm(g_net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "            g_net = tf.layers.conv2d(g_net, gf_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generator3/res/conv2d3')\n",
    "            #g_net = tf.layers.batch_normalization(g_net, training = self.train, gamma_initializer = gamma_init, name='generator3/batch_norm3')\n",
    "            g_net = tf.contrib.layers.batch_norm(g_net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = None)\n",
    "            g_net3 = tf.add(g_net, g_net2)\n",
    "            g_net3 = tf.nn.relu(g_net3)\n",
    "            \n",
    "            #####4\n",
    "            g_net4 = tf.image.resize_images(g_net3, [16, 16], method = 1, align_corners = False)\n",
    "            g_net4 = tf.layers.conv2d(g_net4, gf_dim*2, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generator4/conv2d1')\n",
    "            #g_net4 = tf.layers.batch_normalization(g_net4, training = self.train, gamma_initializer = gamma_init, name='generator4/batch_norm1')\n",
    "            g_net4 = tf.contrib.layers.batch_norm(g_net4, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "            \n",
    "            #####5\n",
    "            g_net5 = tf.image.resize_images(g_net4, [32, 32], method = 1, align_corners = False)\n",
    "            g_net5 = tf.layers.conv2d(g_net5, gf_dim, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generator5/conv2d1')\n",
    "            #g_net5 = tf.layers.batch_normalization(g_net5, training = self.train, gamma_initializer = gamma_init, name='generator5/batch_norm1')\n",
    "            g_net5 = tf.contrib.layers.batch_norm(g_net5, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "            \n",
    "            #####output\n",
    "            g_neto = tf.image.resize_images(g_net5, [64, 64], method = 1, align_corners = False)\n",
    "            g_neto = tf.layers.conv2d(g_neto, 3, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'generatoro/conv2d1')\n",
    "            \n",
    "            \n",
    "            self.generator_net = g_neto\n",
    "            self.outputs = tf.nn.tanh(g_neto)/2+0.5\n",
    "            self.logits = g_neto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet structure\n",
    "class Generator2:\n",
    "    def __init__(self, image, text, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.df_dim = 128 # 196 for MSCOCO\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):        \n",
    "        with tf.variable_scope('discriminator2', reuse=self.reuse):\n",
    "            kernal_init = tf.random_normal_initializer(stddev=0.02)\n",
    "            gamma_init=tf.random_normal_initializer(1., 0.02)\n",
    "            df_dim = 64\n",
    "            \n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM'], name='discrim/text_input', reuse=self.reuse)\n",
    "            text_input = tf.nn.leaky_relu(text_input, 0.2)\n",
    "            text_input = tf.expand_dims(text_input, axis=1)\n",
    "            text_input = tf.expand_dims(text_input, axis=1)\n",
    "            text_input = tf.tile(text_input, multiples=[1,16,16,1])\n",
    "            \n",
    "            \n",
    "            net1 = tf.layers.conv2d(net0, df_dim, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim1/image/conv2d1')\n",
    "            net1 = tf.nn.relu(net1)\n",
    "            net2 = tf.layers.conv2d(net1, df_dim*2, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/image/conv2d1')\n",
    "            #net2 = tf.layers.batch_normalization(net2, training = self.train, gamma_initializer = gamma_init, name='discrim2/image/batch_norm1')\n",
    "            net2 = tf.contrib.layers.batch_norm(net2, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net2 = tf.nn.relu(net2)\n",
    "            net3 = tf.layers.conv2d(net2, df_dim*4, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim3/image/conv2d1')\n",
    "            #net3 = tf.layers.batch_normalization(net3, training = self.train, gamma_initializer = gamma_init, name='discrim3/image/batch_norm1')\n",
    "            net3 = tf.contrib.layers.batch_norm(net3, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net3 = tf.nn.relu(net3)\n",
    "            \n",
    "            img_text_concate = tf.concat([text_input, net4], axis=3, name='discrim/concate')\n",
    "            d_net = tf.layers.conv2d(d_net, df_dim*4, (3, 3), (1, 1), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim/conv2d1')\n",
    "            #d_net = tf.layers.batch_normalization(d_net, training = self.train, gamma_initializer = gamma_init, name='discrim/batch_norm1')\n",
    "            d_net = tf.contrib.layers.batch_norm(d_net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            #########residual\n",
    "            net = tf.layers.conv2d(d_net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual1/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net = tf.nn.relu(net)\n",
    "            net = tf.layers.conv2d(net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual1/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.add(d_net, net)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            #########residual\n",
    "            net = tf.layers.conv2d(d_net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual2/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net = tf.nn.relu(net)\n",
    "            net = tf.layers.conv2d(net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual2/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.add(d_net, net)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            #########residual\n",
    "            net = tf.layers.conv2d(d_net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual3/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net = tf.nn.relu(net)\n",
    "            net = tf.layers.conv2d(net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual3/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.add(d_net, net)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            #########residual\n",
    "            net = tf.layers.conv2d(d_net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual4/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net = tf.nn.relu(net)\n",
    "            net = tf.layers.conv2d(net, df_dim*4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/residual4/conv2d1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.add(d_net, net)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            ########upsampling\n",
    "            d_net = tf.image.resize_nearest_neighbor(d_net, [32, 32])\n",
    "            d_net = tf.layers.conv2d(d_net, df_dim*2, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/upsampling1/conv2d1')\n",
    "            d_net = tf.contrib.layers.batch_norm(d_net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            d_net = tf.image.resize_nearest_neighbor(d_net, [64, 64])\n",
    "            d_net = tf.layers.conv2d(d_net, df_dim, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/upsampling2/conv2d1')\n",
    "            d_net = tf.contrib.layers.batch_norm(d_net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            d_net = tf.image.resize_nearest_neighbor(d_net, [128, 128])\n",
    "            d_net = tf.layers.conv2d(d_net, df_dim // 2, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/upsampling3/conv2d1')\n",
    "            d_net = tf.contrib.layers.batch_norm(d_net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            d_net = tf.image.resize_nearest_neighbor(d_net, [256, 256])\n",
    "            d_net = tf.layers.conv2d(d_net, df_dim // 4, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/upsampling4/conv2d1')\n",
    "            d_net = tf.contrib.layers.batch_norm(d_net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.nn.relu(d_net)\n",
    "            \n",
    "            d_neto = tf.layers.conv2d(d_net, 3, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/upsampling4/conv2d1')\n",
    "            \n",
    "            self.logits = d_neto\n",
    "            net_output = tf.nn.tanh(d_neto)\n",
    "            self.discriminator_net = net_output\n",
    "            self.outputs = net_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet structure\n",
    "class Discriminator:\n",
    "    def __init__(self, image, text, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.df_dim = 128 # 196 for MSCOCO\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):        \n",
    "        with tf.variable_scope('discriminator', reuse=self.reuse):\n",
    "            kernal_init = tf.truncated_normal_initializer(stddev=0.02)\n",
    "            gamma_init=tf.random_normal_initializer(1., 0.02)\n",
    "            beta_init=tf.zeros_initializer()\n",
    "            moving_mean_init=tf.zeros_initializer()\n",
    "            moving_variance_init=tf.ones_initializer()\n",
    "            param_initializers = {'beta': beta_init,\n",
    "                                 'gamma': gamma_init,\n",
    "                                 'moving_mean': moving_mean_init,\n",
    "                                 'moving_variance': moving_variance_init}\n",
    "            df_dim = 64\n",
    "            \n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM'], name='discrim/text_input', reuse=self.reuse)\n",
    "            text_input = tf.nn.leaky_relu(text_input, 0.2)\n",
    "            text_input = tf.expand_dims(text_input, axis=1)\n",
    "            text_input = tf.expand_dims(text_input, axis=1)\n",
    "            text_input = tf.tile(text_input, multiples=[1,4,4,1])\n",
    "            \n",
    "            #print(self.image.shape)\n",
    "            #print(text_input.shape)\n",
    "            \n",
    "            #image_flatten = tf.contrib.layers.flatten(self.image)\n",
    "            #image_input = tf.layers.dense(image_flatten, self.hparas['TEXT_DIM'], name='discrim/image_input', reuse=self.reuse)\n",
    "            net0 = tf.layers.conv2d(self.image, df_dim, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim0/image/conv2d1')\n",
    "            \n",
    "            net1 = tf.layers.conv2d(net0, df_dim*2, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim1/image/conv2d1')\n",
    "            #net1 = tf.layers.batch_normalization(net1, training = self.train, gamma_initializer = gamma_init, name='discrim1/image/batch_norm1')\n",
    "            net1 = tf.contrib.layers.batch_norm(net1, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = lambda x: tf.nn.leaky_relu(x, alpha=0.2))\n",
    "            net2 = tf.layers.conv2d(net1, df_dim*4, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim2/image/conv2d1')\n",
    "            #net2 = tf.layers.batch_normalization(net2, training = self.train, gamma_initializer = gamma_init, name='discrim2/image/batch_norm1')\n",
    "            net2 = tf.contrib.layers.batch_norm(net2, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = lambda x: tf.nn.leaky_relu(x, alpha=0.2))\n",
    "            net3 = tf.layers.conv2d(net2, df_dim*8, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim3/image/conv2d1')\n",
    "            #net3 = tf.layers.batch_normalization(net3, training = self.train, gamma_initializer = gamma_init, name='discrim3/image/batch_norm1')\n",
    "            net1 = tf.contrib.layers.batch_norm(net1, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = None)\n",
    "            \n",
    "            net = tf.layers.conv2d(net3, df_dim*2, (1, 1), (1, 1), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim4/image/conv2d1')\n",
    "            #net = tf.layers.batch_normalization(net, training = self.train, gamma_initializer = gamma_init, name='discrim4/image/batch_norm1')\n",
    "            net = tf.contrib.layers.batch_norm(net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = lambda x: tf.nn.leaky_relu(x, alpha=0.2))\n",
    "            net = tf.layers.conv2d(net, df_dim*2, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim4/image/conv2d2')\n",
    "            #net = tf.layers.batch_normalization(net, training = self.train, gamma_initializer = gamma_init, name='discrim4/image/batch_norm2')\n",
    "            net = tf.contrib.layers.batch_norm(net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = lambda x: tf.nn.leaky_relu(x, alpha=0.2))\n",
    "            net = tf.layers.conv2d(net, df_dim*8, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim4/image/conv2d3')\n",
    "            #net = tf.layers.batch_normalization(net, training = self.train, gamma_initializer = gamma_init, name='discrim4/image/batch_norm3')\n",
    "            net = tf.contrib.layers.batch_norm(net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = None)\n",
    "            \n",
    "            net4 = tf.add(net, net3)\n",
    "            net4 = tf.nn.leaky_relu(net4, 0.2)\n",
    "            \n",
    "            #print(net4.shape)\n",
    "            \n",
    "            img_text_concate = tf.concat([text_input, net4], axis=3, name='discrim/concate')\n",
    "            d_net = tf.layers.dense(img_text_concate, 1, name='discrim/d_net', reuse=self.reuse)\n",
    "            d_net = tf.layers.conv2d(d_net, df_dim*8, (1, 1), (1, 1), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                name = 'discrim/conv2d1')\n",
    "            #d_net = tf.layers.batch_normalization(d_net, training = self.train, gamma_initializer = gamma_init, name='discrim/batch_norm1')\n",
    "            d_net = tf.contrib.layers.batch_norm(d_net, param_initializers = param_initializers, is_training = self.train, updates_collections = None,\n",
    "                                                 activation_fn = lambda x: tf.nn.leaky_relu(x, alpha=0.2))\n",
    "            #print(d_net.shape)\n",
    "            d_neto = tf.layers.conv2d(d_net, 1, (4, 4), (4, 4), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                name = 'discrimo/conv2d1')\n",
    "            #print(d_neto.shape)\n",
    "            \n",
    "            self.logits = d_neto\n",
    "            net_output = tf.nn.sigmoid(d_neto)\n",
    "            self.discriminator_net = net_output\n",
    "            self.outputs = net_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet structure\n",
    "class Discriminator2:\n",
    "    def __init__(self, image, text, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.df_dim = 128 # 196 for MSCOCO\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):        \n",
    "        with tf.variable_scope('discriminator2', reuse=self.reuse):\n",
    "            kernal_init = tf.random_normal_initializer(stddev=0.02)\n",
    "            gamma_init=tf.random_normal_initializer(1., 0.02)\n",
    "            df_dim = 64\n",
    "            \n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM'], name='discrim/text_input', reuse=self.reuse)\n",
    "            text_input = tf.nn.leaky_relu(text_input, 0.2)\n",
    "            text_input = tf.expand_dims(text_input, axis=1)\n",
    "            text_input = tf.expand_dims(text_input, axis=1)\n",
    "            text_input = tf.tile(text_input, multiples=[1,4,4,1])\n",
    "            \n",
    "            \n",
    "            \n",
    "            #image_flatten = tf.contrib.layers.flatten(self.image)\n",
    "            #image_input = tf.layers.dense(image_flatten, self.hparas['TEXT_DIM'], name='discrim/image_input', reuse=self.reuse)\n",
    "            net0 = tf.layers.conv2d(self.image, df_dim, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim0/image/conv2d1')\n",
    "            \n",
    "            net1 = tf.layers.conv2d(net0, df_dim*2, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim1/image/conv2d1')\n",
    "            #net1 = tf.layers.batch_normalization(net1, training = self.train, gamma_initializer = gamma_init, name='discrim1/image/batch_norm1')\n",
    "            net1 = tf.contrib.layers.batch_norm(net1, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net1 = tf.nn.leaky_relu(net1, 0.2)\n",
    "            net2 = tf.layers.conv2d(net1, df_dim*4, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim2/image/conv2d1')\n",
    "            #net2 = tf.layers.batch_normalization(net2, training = self.train, gamma_initializer = gamma_init, name='discrim2/image/batch_norm1')\n",
    "            net2 = tf.contrib.layers.batch_norm(net2, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net2 = tf.nn.leaky_relu(net2, 0.2)\n",
    "            net3 = tf.layers.conv2d(net2, df_dim*8, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim3/image/conv2d1')\n",
    "            \n",
    "            #net3 = tf.layers.batch_normalization(net3, training = self.train, gamma_initializer = gamma_init, name='discrim3/image/batch_norm1')\n",
    "            net3 = tf.contrib.layers.batch_norm(net3, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net3 = tf.nn.leaky_relu(net3, 0.2)\n",
    "            \n",
    "            net4 = tf.layers.conv2d(net3, df_dim*16, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim4/image/conv2d1')\n",
    "            net4 = tf.contrib.layers.batch_norm(net4, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net4 = tf.nn.leaky_relu(net4, 0.2)\n",
    "            \n",
    "            net5 = tf.layers.conv2d(net4, df_dim*32, (4, 4), (2, 2), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim5/image/conv2d1')\n",
    "            net5 = tf.contrib.layers.batch_norm(net5, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net5 = tf.nn.leaky_relu(net5, 0.2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            net = tf.layers.conv2d(net5, df_dim*2, (1, 1), (1, 1), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim5/image/conv2d2')\n",
    "            #net = tf.layers.batch_normalization(net, training = self.train, gamma_initializer = gamma_init, name='discrim4/image/batch_norm1')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net = tf.nn.leaky_relu(net, 0.2)\n",
    "            net = tf.layers.conv2d(net, df_dim*2, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim5/image/conv2d3')\n",
    "            #net = tf.layers.batch_normalization(net, training = self.train, gamma_initializer = gamma_init, name='discrim4/image/batch_norm2')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            net = tf.nn.leaky_relu(net, 0.2)\n",
    "            net = tf.layers.conv2d(net, df_dim*8, (3, 3), (1, 1), padding = 'SAME', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim5/image/conv2d4')\n",
    "            #net = tf.layers.batch_normalization(net, training = self.train, gamma_initializer = gamma_init, name='discrim4/image/batch_norm3')\n",
    "            net = tf.contrib.layers.batch_norm(net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            \n",
    "            net6 = tf.add(net, net5)\n",
    "            net6 = tf.nn.leaky_relu(net6, 0.2)\n",
    "            \n",
    "            img_text_concate = tf.concat([text_input, net6], axis=3, name='discrim/concate')\n",
    "            d_net = tf.layers.dense(img_text_concate, 1, name='discrim/d_net', reuse=self.reuse)\n",
    "            d_net = tf.layers.conv2d(d_net, df_dim*8, (1, 1), (1, 1), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrim/conv2d1')\n",
    "            #d_net = tf.layers.batch_normalization(d_net, training = self.train, gamma_initializer = gamma_init, name='discrim/batch_norm1')\n",
    "            d_net = tf.contrib.layers.batch_norm(d_net, decay=0.9, center=True, scale=True,is_training = self.train, updates_collections = None, zero_debias_moving_mean = True)\n",
    "            d_net = tf.nn.leaky_relu(d_net, 0.2)\n",
    "    \n",
    "            d_neto = tf.layers.conv2d(d_net, 1, (4, 4), (4, 4), padding = 'VALID', kernel_initializer = kernal_init,\n",
    "                                bias_initializer = None, name = 'discrimo/conv2d1')\n",
    "   \n",
    "            self.logits = d_neto\n",
    "            net_output = tf.nn.sigmoid(d_neto)\n",
    "            self.discriminator_net = net_output\n",
    "            self.outputs = net_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss(mean, log_sigma):\n",
    "    kl_loss = -log_sigma + .5 * (-1 + tf.exp(2. * log_sigma) + tf.square(mean))\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    return kl_loss\n",
    "class Conditioning_Augmentation:\n",
    "    def __init__(self, text, hparas, reuse, layer):\n",
    "        self.text = text\n",
    "        self.reuse = reuse\n",
    "        self.hparas = hparas\n",
    "        self.layer = layer\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('layer' + str(self.layer) + '/ca', reuse = self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            conditions = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM']*2, activation=lambda x: tf.nn.leaky_relu(x, alpha=0.2),\n",
    "                                        name='ca/text_input', reuse=self.reuse)\n",
    "            mean = conditions[:, :self.hparas['TEXT_DIM']]\n",
    "            log_sigma = conditions[:, self.hparas['TEXT_DIM']:]\n",
    "            epsilon = tf.truncated_normal(tf.shape(mean))\n",
    "            stddev = tf.exp(log_sigma)\n",
    "#             print(epsilon.shape)\n",
    "#             print(stddev.shape)\n",
    "            c = mean + stddev * epsilon\n",
    "            kl_loss = KL_loss(mean, log_sigma)\n",
    "            self.outputs = c\n",
    "            self.kl_loss = kl_loss * self.hparas['KL_LOSS_COEFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparas():\n",
    "    hparas = {\n",
    "        'MAX_SEQ_LENGTH' : 20,\n",
    "        'EMBED_DIM' : 200, # word embedding dimension\n",
    "        'VOCAB_SIZE' : len(vocab),\n",
    "        'TEXT_DIM' : 200, # text embedding dimension\n",
    "        'RNN_HIDDEN_SIZE' : 50,\n",
    "        'KEEP_PROB' : 0.7,\n",
    "        'Z_DIM' : 128, # random noise z dimension\n",
    "        'IMAGE_SIZE' : [64, 64, 3], # render image size\n",
    "        'BATCH_SIZE' : 64,\n",
    "        'LR' : 0.0002,\n",
    "        'LR_DECAY': 0.5,\n",
    "        'KL_LOSS_COEFF': 1, \n",
    "        'DECAY_EVERY_EPOCH':100,\n",
    "        'BETA' : 0.5, # AdamOptimizer parameter\n",
    "        'N_EPOCH' : 200,\n",
    "        'N_SAMPLE' : num_training_sample\n",
    "    }\n",
    "    return hparas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, hparas, training_phase, dataset_path, ckpt_path, inference_path, recover=None):\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.dataset_path = dataset_path # dataPath+'/text2ImgData.pkl'\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.sample_path = './samples'\n",
    "        self.inference_path = './inference'\n",
    "        \n",
    "        self._get_session() # get session\n",
    "        self._get_train_data_iter() # initialize and get data iterator\n",
    "        self._input_layer() # define input placeholder\n",
    "        self._get_inference() # build generator and discriminator\n",
    "        self._get_loss() # define gan loss\n",
    "        self._get_var_with_name() # get variables for each part of model\n",
    "        self._optimize() # define optimizer\n",
    "        self._init_vars()\n",
    "        self._get_saver()\n",
    "        \n",
    "        if recover is not None:\n",
    "            self._load_checkpoint(recover)\n",
    "            \n",
    "            \n",
    "        \n",
    "    def _get_train_data_iter(self):\n",
    "        if self.train: # training data iteratot\n",
    "            iterator_train, types, shapes = data_iterator(self.dataset_path+'/text2ImgData.pkl',\n",
    "                                                          self.hparas['BATCH_SIZE'], training_data_generator)\n",
    "            iter_initializer = iterator_train.initializer\n",
    "            self.next_element = iterator_train.get_next()\n",
    "            self.sess.run(iterator_train.initializer)\n",
    "            self.iterator_train = iterator_train\n",
    "        else: # testing data iterator\n",
    "            iterator_test, types, shapes = data_iterator_test(self.dataset_path+'/testData.pkl', self.hparas['BATCH_SIZE'])\n",
    "            iter_initializer = iterator_test.initializer\n",
    "            self.next_element = iterator_test.get_next()\n",
    "            self.sess.run(iterator_test.initializer)\n",
    "            self.iterator_test = iterator_test\n",
    "            \n",
    "    def _input_layer(self):\n",
    "        if self.train:\n",
    "            self.real_image = tf.placeholder('float32',\n",
    "                                              [self.hparas['BATCH_SIZE'], self.hparas['IMAGE_SIZE'][0],\n",
    "                                               self.hparas['IMAGE_SIZE'][1], self.hparas['IMAGE_SIZE'][2]],\n",
    "                                              name='real_image')\n",
    "            self.caption = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            self.z_noise = tf.placeholder(tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']], name='z_noise')\n",
    "        else:\n",
    "            self.caption = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            self.z_noise = tf.placeholder(tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']], name='z_noise')\n",
    "    \n",
    "    def _get_inference(self):\n",
    "        if self.train:\n",
    "            self.real_image = tf.image.random_flip_left_right(self.real_image)\n",
    "            self.real_image = tf.image.random_brightness(self.real_image, 0.2)\n",
    "            # GAN training\n",
    "            # encoding text\n",
    "            \n",
    "            text_encoder = TextEncoder(self.caption, hparas = self.hparas, training_phase=True, reuse=False)\n",
    "            self.wrong_caption = tf.random.shuffle(self.caption)\n",
    "            self.text_encoder = text_encoder\n",
    "            #encoding image\n",
    "            image_encoder = ImageEncoder(self.real_image, hparas = self.hparas, training_phase = True, reuse = False)\n",
    "            self.image_encoder = image_encoder\n",
    "            \n",
    "            \n",
    "            #wrong caption\n",
    "            self.wrong_caption = tf.random.shuffle(self.caption)\n",
    "            self.wrong_text_encoder = TextEncoder(self.wrong_caption, hparas = self.hparas, training_phase=True, reuse=True)\n",
    "            #wrong image\n",
    "            self.wrong_image = tf.random.shuffle(self.real_image)\n",
    "            self.wrong_image_encoder = ImageEncoder(self.wrong_image, hparas = self.hparas, training_phase=True, reuse = True)\n",
    "            \n",
    "            \n",
    "            text_encoder = TextEncoder(self.caption, hparas = self.hparas, training_phase=False, reuse=True)\n",
    "            \n",
    "            #########First Layer\n",
    "            \n",
    "            # generating image\n",
    "            self.L1_CA = Conditioning_Augmentation(text_encoder.outputs, self.hparas, reuse = False, layer = 1)\n",
    "            generator = Generator(self.z_noise, self.L1_CA.outputs, training_phase=True,\n",
    "                                  hparas=self.hparas, reuse=False)\n",
    "            self.generator = generator\n",
    "            \n",
    "            # discriminize\n",
    "            # fake image real caption\n",
    "            fake_discriminator = Discriminator(generator.outputs, text_encoder.outputs,\n",
    "                                               training_phase=True, hparas=self.hparas, reuse=False)\n",
    "            self.fake_discriminator = fake_discriminator\n",
    "            # real image real caption\n",
    "            real_discriminator = Discriminator(self.real_image, text_encoder.outputs, training_phase=True,\n",
    "                                              hparas=self.hparas, reuse=True)\n",
    "            self.real_discriminator = real_discriminator\n",
    "            # real image fake caption\n",
    "            real_fake_discriminator = Discriminator(self.wrong_image, text_encoder.outputs, training_phase=True,\n",
    "                                              hparas=self.hparas, reuse=True)\n",
    "            self.real_fake_discriminator = real_fake_discriminator\n",
    "            \n",
    "            #########Second Layer\n",
    "            \n",
    "            \n",
    "        else: # inference mode\n",
    "            \n",
    "            self.text_embed = TextEncoder(self.caption, hparas=self.hparas, training_phase=False, reuse=False)\n",
    "            self.ca1 = Conditioning_Augmentation(self.text_embed.outputs, self.hparas, reuse = False, layer = 1)\n",
    "            self.generate_image_net = Generator(self.z_noise, self.ca1.outputs, training_phase=False,\n",
    "                                                hparas=self.hparas, reuse=False)\n",
    "    def _get_loss(self):\n",
    "        if self.train:\n",
    "#             d_loss1 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_discriminator.logits,\n",
    "#                                                                               labels=tf.ones_like(self.real_discriminator.logits),\n",
    "#                                                                               name='d_loss1'))\n",
    "#             d_loss2 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "#                                                                               labels=tf.zeros_like(self.fake_discriminator.logits),\n",
    "#                                                                               name='d_loss2'))\n",
    "#             self.d_loss = d_loss1 + d_loss2\n",
    "#             self.g_loss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "#                                                                                   labels=tf.ones_like(self.fake_discriminator.logits),\n",
    "#                                                                                   name='g_loss'))\n",
    "            rnn_t = self.text_encoder.outputs\n",
    "            rnn_f = self.wrong_text_encoder.outputs\n",
    "            cnn_t = self.image_encoder.outputs\n",
    "            cnn_f = self.wrong_image_encoder.outputs\n",
    "            alpha = 0.2\n",
    "            self.rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(rnn_t, cnn_t) + cosine_similarity(rnn_t, cnn_f))) + \\\n",
    "                tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(rnn_t, cnn_t) + cosine_similarity(rnn_f, cnn_t)))\n",
    "            \n",
    "\n",
    "            d_loss1 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_discriminator.logits,\n",
    "                                                                              labels=tf.ones_like(self.real_discriminator.logits),\n",
    "                                                                              name='d_loss1'))\n",
    "            d_loss2 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "                                                                              labels=tf.zeros_like(self.fake_discriminator.logits),\n",
    "                                                                              name='d_loss2'))\n",
    "            d_loss3 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_fake_discriminator.logits,\n",
    "                                                                              labels=tf.zeros_like(self.real_fake_discriminator.logits),\n",
    "                                                                              name='d_loss3'))\n",
    "            \n",
    "            self.d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n",
    "            self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "                                                                                  labels=tf.ones_like(self.fake_discriminator.logits),\n",
    "                                                                                  name='g_loss'))\n",
    "            kl_loss = self.L1_CA.kl_loss\n",
    "            self.g_loss += kl_loss\n",
    "#             epsilon = tf.random_uniform([], 0.0, 1.0, dtype = tf.float32)\n",
    "#             x_hat = epsilon * self.real_image + (1.0 - epsilon) * self.generator.outputs\n",
    "#             x_hat_discriminator = Discriminator(x_hat, self.text_encoder.outputs, training_phase=True,\n",
    "#                                               hparas=self.hparas, reuse=True)\n",
    "#             gradients = tf.gradients(x_hat_discriminator.logits, x_hat)[0]\n",
    "#             lamda = 10\n",
    "#             gradient_penalty = lamda * tf.reduce_mean((tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1])) - 1.0)**2)\n",
    "#             #gradient_penalty = lamda * tf.reduce_mean((tf.norm(gradients) - 1.0)**2)\n",
    "#             self.d_loss += gradient_penalty   \n",
    "    \n",
    "    def _optimize(self):\n",
    "        if self.train:\n",
    "            with tf.variable_scope('learning_rate'):\n",
    "                self.lr_var = tf.Variable(self.hparas['LR'], trainable=False)\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            generator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            \n",
    "#             gvs = discriminator_optimizer.compute_gradients(self.d_loss)\n",
    "#             capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "#             self.d_optim = discriminator_optimizer.apply_gradients(capped_gvs)\n",
    "            \n",
    "#             gvs = generator_optimizer.compute_gradients(self.g_loss)\n",
    "#             capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "#             self.g_optim = generator_optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "            self.d_optim = discriminator_optimizer.minimize(self.d_loss, var_list=self.discrim_vars)\n",
    "            self.g_optim = generator_optimizer.minimize(self.g_loss, var_list=self.generator_vars+self.text_encoder_vars+self.ca1_vars)\n",
    "        \n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.rnn_loss, self.image_encoder_vars + self.text_encoder_vars), 10)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            self.rnn_optim = optimizer.apply_gradients(zip(grads, self.image_encoder_vars + self.text_encoder_vars))\n",
    "        \n",
    "    def training(self):\n",
    "        \n",
    "        for _epoch in range(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "            # Update learning rate\n",
    "            if _epoch != 0 and (_epoch % self.hparas['DECAY_EVERY_EPOCH'] == 0):\n",
    "                self.lr_decay = self.hparas['LR_DECAY'] ** (_epoch // self.hparas['DECAY_EVERY_EPOCH'])\n",
    "                self.sess.run(tf.assign(self.lr_var, self.hparas['LR'] * self.lr_decay))\n",
    "            \n",
    "            n_batch_epoch = int(self.hparas['N_SAMPLE']/self.hparas['BATCH_SIZE'])\n",
    "            for _step in range(n_batch_epoch):\n",
    "                step_time = time.time()\n",
    "                image_batch, caption_batch = self.sess.run(self.next_element)\n",
    "                b_z = np.random.normal(loc=0.0, scale=1.0, \n",
    "                                       size=(self.hparas['BATCH_SIZE'], self.hparas['Z_DIM'])).astype(np.float32)\n",
    "                if _epoch < 200:\n",
    "                    self.encoder_error, _ = self.sess.run([self.rnn_loss, self.rnn_optim],\n",
    "                                                         feed_dict={\n",
    "                                                             self.real_image:image_batch,\n",
    "                                                             self.caption:caption_batch\n",
    "                                                         })\n",
    "                else:\n",
    "                    self.encoder_error = 0\n",
    "                \n",
    "                # update discriminator\n",
    "                self.discriminator_error, _ = self.sess.run([self.d_loss, self.d_optim],\n",
    "                                                           feed_dict={\n",
    "                                                                self.real_image:image_batch,\n",
    "                                                                self.caption:caption_batch,\n",
    "                                                                self.z_noise:b_z})\n",
    "\n",
    "                # update generate\n",
    "                self.generator_error, _ = self.sess.run([self.g_loss, self.g_optim],\n",
    "                                                       feed_dict={self.caption: caption_batch, self.z_noise : b_z})\n",
    "                \n",
    "                if _step%50==0:\n",
    "                    print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.3f, g_loss: %.3f, rnn_loss: %.3f\" \\\n",
    "                            % (_epoch, self.hparas['N_EPOCH'], _step, n_batch_epoch, time.time() - step_time,\n",
    "                               self.discriminator_error, self.generator_error, self.encoder_error))\n",
    "            if _epoch != 0 and (_epoch+1)%5==0:\n",
    "                self._save_checkpoint(_epoch)\n",
    "                self._sample_visiualize(_epoch)\n",
    "            \n",
    "    def inference(self):\n",
    "        for _iters in range(100):\n",
    "            caption, idx = self.sess.run(self.next_element)\n",
    "            z_seed = np.random.normal(loc=0.0, scale=1.0, size=(self.hparas['BATCH_SIZE'], self.hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "            img_gen, rnn_out = self.sess.run([self.generate_image_net.outputs, self.text_embed.outputs],\n",
    "                                             feed_dict={self.caption : caption, self.z_noise : z_seed})\n",
    "            for i in range(self.hparas['BATCH_SIZE']):\n",
    "                scipy.misc.imsave(self.inference_path+'/inference_{:04d}.png'.format(idx[i]), img_gen[i])\n",
    "                \n",
    "    def test_pred(self, caption):\n",
    "        z_seed = np.random.normal(loc=0.0, scale=1.0, size=(self.hparas['BATCH_SIZE'], self.hparas['Z_DIM'])).astype(np.float32)\n",
    "        text_embed = TextEncoder(self.caption, hparas=self.hparas, training_phase=False, reuse=True)\n",
    "        generate_image_net = Generator(self.z_noise, text_embed.outputs, training_phase=False,\n",
    "                                                hparas=self.hparas, reuse=True)\n",
    "        img_gen, rnn_out = self.sess.run([generate_image_net.outputs, text_embed.outputs],\n",
    "                                             feed_dict={self.caption : caption, self.z_noise : z_seed})\n",
    "        \n",
    "        for i in range(self.hparas['BATCH_SIZE']):\n",
    "            scipy.misc.imsave(self.inference_path+'/inference_123123123_{:04d}.png'.format(i), img_gen[i])\n",
    "        \n",
    "#         print(img_gen.shape)\n",
    "        \n",
    "#         norm_img_gen = img_gen / 2 + 0.5\n",
    "#         print(norm_img_gen.max(),norm_img_gen.min())\n",
    "# #         for i in range(3):\n",
    "#         plt.imshow(norm_img_gen[1])\n",
    "                \n",
    "    def _init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _get_session(self):\n",
    "        self.sess = tf.Session()\n",
    "    \n",
    "    def _get_saver(self):\n",
    "        if self.train:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.cnn_saver = tf.train.Saver(var_list=self.image_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            self.d_saver = tf.train.Saver(var_list=self.discrim_vars)\n",
    "            self.ca1_saver = tf.train.Saver(var_list=self.ca1_vars)\n",
    "        else:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            self.ca1_saver = tf.train.Saver(var_list=self.ca1_vars)\n",
    "            \n",
    "    def _sample_visiualize(self, epoch):\n",
    "        ni = int(np.ceil(np.sqrt(self.hparas['BATCH_SIZE'])))\n",
    "        sample_size = self.hparas['BATCH_SIZE']\n",
    "        max_len = self.hparas['MAX_SEQ_LENGTH']\n",
    "        \n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, self.hparas['Z_DIM'])).astype(np.float32)\n",
    "        sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"]*int(sample_size/ni) + [\"this flower has petals that are yellow, white and purple and has dark lines\"]*int(sample_size/ni) + [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) + [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "        for i, sent in enumerate(sample_sentence):\n",
    "            sample_sentence[i] = sent2IdList(sent, max_len)\n",
    "            \n",
    "        img_gen, rnn_out = self.sess.run([self.generator.outputs, self.text_encoder.outputs],\n",
    "                                         feed_dict={self.caption : sample_sentence, self.z_noise : sample_seed})\n",
    "        save_images(img_gen, [ni, ni], self.sample_path+'/train_{:02d}.png'.format(epoch))\n",
    "        \n",
    "    def _get_var_with_name(self):\n",
    "        #t_vars = tf.trainable_variables()\n",
    "        t_vars = tf.global_variables()\n",
    "\n",
    "        self.text_encoder_vars = [var for var in t_vars if 'rnnftxt' in var.name]\n",
    "        self.image_encoder_vars = [var for var in t_vars if 'cnnftxt' in var.name]\n",
    "        self.generator_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        self.discrim_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "        self.ca1_vars = [var for var in t_vars if 'layer1/ca' in var.name]\n",
    "    \n",
    "    def _load_checkpoint(self, recover):\n",
    "        if self.train:\n",
    "            self.rnn_saver.restore(self.sess, self.ckpt_path+'rnn_model_'+str(recover)+'.ckpt')\n",
    "            self.cnn_saver.restore(self.sess, self.ckpt_path+'cnn_model_'+str(recover)+'.ckpt')\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "            self.d_saver.restore(self.sess, self.ckpt_path+'d_model_'+str(recover)+'.ckpt')\n",
    "            self.ca1_saver.restore(self.sess, self.ckpt_path+'ca_model_'+str(recover)+'.ckpt')\n",
    "        else:\n",
    "            self.rnn_saver.restore(self.sess, self.ckpt_path+'rnn_model_'+str(recover)+'.ckpt')\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "            self.ca1_saver.restore(self.sess, self.ckpt_path+'ca_model_'+str(recover)+'.ckpt')\n",
    "        print('-----success restored checkpoint--------')\n",
    "    \n",
    "    def _save_checkpoint(self, epoch):\n",
    "        self.rnn_saver.save(self.sess, self.ckpt_path+'rnn_model_'+str(epoch)+'.ckpt')\n",
    "        self.cnn_saver.save(self.sess, self.ckpt_path+'cnn_model_'+str(epoch)+'.ckpt')\n",
    "        self.g_saver.save(self.sess, self.ckpt_path+'g_model_'+str(epoch)+'.ckpt')\n",
    "        self.d_saver.save(self.sess, self.ckpt_path+'d_model_'+str(epoch)+'.ckpt')\n",
    "        self.ca1_saver.save(self.sess, self.ckpt_path+'ca_model_'+str(epoch)+'.ckpt')\n",
    "        print('-----success saved checkpoint--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0/200] [   0/ 115] time: 2.7149s, d_loss: 1.482, g_loss: 0.689, rnn_loss: 0.431\n",
      "Epoch: [ 0/200] [  50/ 115] time: 0.4519s, d_loss: 1.168, g_loss: 2.352, rnn_loss: 0.298\n",
      "Epoch: [ 0/200] [ 100/ 115] time: 0.4518s, d_loss: 1.211, g_loss: 1.647, rnn_loss: 0.356\n",
      "Epoch: [ 1/200] [   0/ 115] time: 0.4508s, d_loss: 1.192, g_loss: 1.731, rnn_loss: 0.353\n",
      "Epoch: [ 1/200] [  50/ 115] time: 0.4333s, d_loss: 1.268, g_loss: 1.667, rnn_loss: 0.223\n",
      "Epoch: [ 1/200] [ 100/ 115] time: 0.4459s, d_loss: 1.130, g_loss: 2.243, rnn_loss: 0.313\n",
      "Epoch: [ 2/200] [   0/ 115] time: 0.4443s, d_loss: 1.512, g_loss: 1.606, rnn_loss: 0.376\n",
      "Epoch: [ 2/200] [  50/ 115] time: 0.4540s, d_loss: 1.294, g_loss: 1.326, rnn_loss: 0.281\n",
      "Epoch: [ 2/200] [ 100/ 115] time: 0.4480s, d_loss: 1.323, g_loss: 1.457, rnn_loss: 0.329\n",
      "Epoch: [ 3/200] [   0/ 115] time: 0.4442s, d_loss: 1.397, g_loss: 1.197, rnn_loss: 0.268\n",
      "Epoch: [ 3/200] [  50/ 115] time: 0.4328s, d_loss: 1.284, g_loss: 1.144, rnn_loss: 0.248\n",
      "Epoch: [ 3/200] [ 100/ 115] time: 0.4528s, d_loss: 1.208, g_loss: 1.486, rnn_loss: 0.301\n",
      "Epoch: [ 4/200] [   0/ 115] time: 0.4508s, d_loss: 1.253, g_loss: 1.213, rnn_loss: 0.243\n",
      "Epoch: [ 4/200] [  50/ 115] time: 0.4718s, d_loss: 1.460, g_loss: 0.771, rnn_loss: 0.251\n",
      "Epoch: [ 4/200] [ 100/ 115] time: 0.4660s, d_loss: 1.197, g_loss: 1.658, rnn_loss: 0.290\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [ 5/200] [   0/ 115] time: 0.4807s, d_loss: 1.169, g_loss: 2.093, rnn_loss: 0.256\n",
      "Epoch: [ 5/200] [  50/ 115] time: 0.4571s, d_loss: 1.242, g_loss: 1.719, rnn_loss: 0.280\n",
      "Epoch: [ 5/200] [ 100/ 115] time: 0.4833s, d_loss: 1.255, g_loss: 1.634, rnn_loss: 0.267\n",
      "Epoch: [ 6/200] [   0/ 115] time: 0.4589s, d_loss: 1.365, g_loss: 1.387, rnn_loss: 0.308\n",
      "Epoch: [ 6/200] [  50/ 115] time: 0.4459s, d_loss: 1.464, g_loss: 1.074, rnn_loss: 0.311\n",
      "Epoch: [ 6/200] [ 100/ 115] time: 0.4505s, d_loss: 1.305, g_loss: 1.568, rnn_loss: 0.336\n",
      "Epoch: [ 7/200] [   0/ 115] time: 0.4472s, d_loss: 1.321, g_loss: 1.457, rnn_loss: 0.343\n",
      "Epoch: [ 7/200] [  50/ 115] time: 0.4483s, d_loss: 1.370, g_loss: 0.999, rnn_loss: 0.245\n",
      "Epoch: [ 7/200] [ 100/ 115] time: 0.4499s, d_loss: 1.388, g_loss: 1.418, rnn_loss: 0.281\n",
      "Epoch: [ 8/200] [   0/ 115] time: 0.4509s, d_loss: 1.242, g_loss: 1.348, rnn_loss: 0.327\n",
      "Epoch: [ 8/200] [  50/ 115] time: 0.4498s, d_loss: 1.344, g_loss: 1.100, rnn_loss: 0.236\n",
      "Epoch: [ 8/200] [ 100/ 115] time: 0.4510s, d_loss: 1.465, g_loss: 1.405, rnn_loss: 0.282\n",
      "Epoch: [ 9/200] [   0/ 115] time: 0.4738s, d_loss: 1.305, g_loss: 1.753, rnn_loss: 0.275\n",
      "Epoch: [ 9/200] [  50/ 115] time: 0.4548s, d_loss: 1.307, g_loss: 1.565, rnn_loss: 0.207\n",
      "Epoch: [ 9/200] [ 100/ 115] time: 0.4508s, d_loss: 1.400, g_loss: 1.091, rnn_loss: 0.292\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [10/200] [   0/ 115] time: 0.4763s, d_loss: 1.323, g_loss: 1.326, rnn_loss: 0.328\n",
      "Epoch: [10/200] [  50/ 115] time: 0.4639s, d_loss: 1.241, g_loss: 1.510, rnn_loss: 0.273\n",
      "Epoch: [10/200] [ 100/ 115] time: 0.4528s, d_loss: 1.306, g_loss: 1.447, rnn_loss: 0.273\n",
      "Epoch: [11/200] [   0/ 115] time: 0.4500s, d_loss: 1.297, g_loss: 0.958, rnn_loss: 0.236\n",
      "Epoch: [11/200] [  50/ 115] time: 0.4551s, d_loss: 1.469, g_loss: 0.785, rnn_loss: 0.207\n",
      "Epoch: [11/200] [ 100/ 115] time: 0.4618s, d_loss: 1.302, g_loss: 1.063, rnn_loss: 0.277\n",
      "Epoch: [12/200] [   0/ 115] time: 0.4608s, d_loss: 1.269, g_loss: 1.646, rnn_loss: 0.244\n",
      "Epoch: [12/200] [  50/ 115] time: 0.4632s, d_loss: 1.290, g_loss: 1.503, rnn_loss: 0.205\n",
      "Epoch: [12/200] [ 100/ 115] time: 0.4475s, d_loss: 1.574, g_loss: 0.654, rnn_loss: 0.256\n",
      "Epoch: [13/200] [   0/ 115] time: 0.4477s, d_loss: 1.340, g_loss: 0.999, rnn_loss: 0.243\n",
      "Epoch: [13/200] [  50/ 115] time: 0.4687s, d_loss: 1.394, g_loss: 1.444, rnn_loss: 0.161\n",
      "Epoch: [13/200] [ 100/ 115] time: 0.4467s, d_loss: 1.380, g_loss: 1.507, rnn_loss: 0.202\n",
      "Epoch: [14/200] [   0/ 115] time: 0.4480s, d_loss: 1.357, g_loss: 1.286, rnn_loss: 0.224\n",
      "Epoch: [14/200] [  50/ 115] time: 0.4584s, d_loss: 1.310, g_loss: 1.537, rnn_loss: 0.121\n",
      "Epoch: [14/200] [ 100/ 115] time: 0.4528s, d_loss: 1.317, g_loss: 1.192, rnn_loss: 0.175\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [15/200] [   0/ 115] time: 0.4852s, d_loss: 1.253, g_loss: 1.895, rnn_loss: 0.147\n",
      "Epoch: [15/200] [  50/ 115] time: 0.4648s, d_loss: 1.268, g_loss: 1.640, rnn_loss: 0.123\n",
      "Epoch: [15/200] [ 100/ 115] time: 0.4454s, d_loss: 1.296, g_loss: 1.406, rnn_loss: 0.139\n",
      "Epoch: [16/200] [   0/ 115] time: 0.4497s, d_loss: 1.335, g_loss: 1.590, rnn_loss: 0.131\n",
      "Epoch: [16/200] [  50/ 115] time: 0.4679s, d_loss: 1.315, g_loss: 1.583, rnn_loss: 0.105\n",
      "Epoch: [16/200] [ 100/ 115] time: 0.4440s, d_loss: 1.289, g_loss: 1.929, rnn_loss: 0.128\n",
      "Epoch: [17/200] [   0/ 115] time: 0.4508s, d_loss: 1.270, g_loss: 1.717, rnn_loss: 0.118\n",
      "Epoch: [17/200] [  50/ 115] time: 0.4682s, d_loss: 1.394, g_loss: 1.039, rnn_loss: 0.089\n",
      "Epoch: [17/200] [ 100/ 115] time: 0.4373s, d_loss: 1.300, g_loss: 1.453, rnn_loss: 0.127\n",
      "Epoch: [18/200] [   0/ 115] time: 0.4471s, d_loss: 1.383, g_loss: 1.124, rnn_loss: 0.111\n",
      "Epoch: [18/200] [  50/ 115] time: 0.4672s, d_loss: 1.438, g_loss: 1.521, rnn_loss: 0.129\n",
      "Epoch: [18/200] [ 100/ 115] time: 0.4419s, d_loss: 1.303, g_loss: 1.796, rnn_loss: 0.110\n",
      "Epoch: [19/200] [   0/ 115] time: 0.4558s, d_loss: 1.374, g_loss: 1.542, rnn_loss: 0.115\n",
      "Epoch: [19/200] [  50/ 115] time: 0.4679s, d_loss: 1.365, g_loss: 1.374, rnn_loss: 0.106\n",
      "Epoch: [19/200] [ 100/ 115] time: 0.4395s, d_loss: 1.363, g_loss: 1.232, rnn_loss: 0.133\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [20/200] [   0/ 115] time: 0.4694s, d_loss: 1.384, g_loss: 0.963, rnn_loss: 0.164\n",
      "Epoch: [20/200] [  50/ 115] time: 0.4699s, d_loss: 1.472, g_loss: 1.061, rnn_loss: 0.089\n",
      "Epoch: [20/200] [ 100/ 115] time: 0.4370s, d_loss: 1.352, g_loss: 1.163, rnn_loss: 0.067\n",
      "Epoch: [21/200] [   0/ 115] time: 0.4493s, d_loss: 1.325, g_loss: 1.310, rnn_loss: 0.094\n",
      "Epoch: [21/200] [  50/ 115] time: 0.4638s, d_loss: 1.529, g_loss: 0.724, rnn_loss: 0.088\n",
      "Epoch: [21/200] [ 100/ 115] time: 0.4412s, d_loss: 1.374, g_loss: 1.001, rnn_loss: 0.081\n",
      "Epoch: [22/200] [   0/ 115] time: 0.4458s, d_loss: 1.314, g_loss: 1.348, rnn_loss: 0.058\n",
      "Epoch: [22/200] [  50/ 115] time: 0.4700s, d_loss: 1.360, g_loss: 1.118, rnn_loss: 0.111\n",
      "Epoch: [22/200] [ 100/ 115] time: 0.4419s, d_loss: 1.336, g_loss: 1.160, rnn_loss: 0.093\n",
      "Epoch: [23/200] [   0/ 115] time: 0.4543s, d_loss: 1.347, g_loss: 1.384, rnn_loss: 0.073\n",
      "Epoch: [23/200] [  50/ 115] time: 0.4715s, d_loss: 1.375, g_loss: 1.054, rnn_loss: 0.068\n",
      "Epoch: [23/200] [ 100/ 115] time: 0.4491s, d_loss: 1.365, g_loss: 1.065, rnn_loss: 0.083\n",
      "Epoch: [24/200] [   0/ 115] time: 0.4568s, d_loss: 1.316, g_loss: 1.508, rnn_loss: 0.084\n",
      "Epoch: [24/200] [  50/ 115] time: 0.4711s, d_loss: 1.457, g_loss: 0.799, rnn_loss: 0.056\n",
      "Epoch: [24/200] [ 100/ 115] time: 0.4483s, d_loss: 1.398, g_loss: 0.750, rnn_loss: 0.061\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [25/200] [   0/ 115] time: 0.4793s, d_loss: 1.426, g_loss: 1.147, rnn_loss: 0.085\n",
      "Epoch: [25/200] [  50/ 115] time: 0.4679s, d_loss: 1.400, g_loss: 0.965, rnn_loss: 0.071\n",
      "Epoch: [25/200] [ 100/ 115] time: 0.4481s, d_loss: 1.413, g_loss: 0.833, rnn_loss: 0.072\n",
      "Epoch: [26/200] [   0/ 115] time: 0.4569s, d_loss: 1.320, g_loss: 1.296, rnn_loss: 0.094\n",
      "Epoch: [26/200] [  50/ 115] time: 0.4701s, d_loss: 1.547, g_loss: 0.716, rnn_loss: 0.058\n",
      "Epoch: [26/200] [ 100/ 115] time: 0.4510s, d_loss: 1.422, g_loss: 1.125, rnn_loss: 0.061\n",
      "Epoch: [27/200] [   0/ 115] time: 0.4566s, d_loss: 1.331, g_loss: 1.261, rnn_loss: 0.099\n",
      "Epoch: [27/200] [  50/ 115] time: 0.4748s, d_loss: 1.558, g_loss: 0.492, rnn_loss: 0.073\n",
      "Epoch: [27/200] [ 100/ 115] time: 0.4520s, d_loss: 1.375, g_loss: 1.044, rnn_loss: 0.047\n",
      "Epoch: [28/200] [   0/ 115] time: 0.4604s, d_loss: 1.359, g_loss: 0.895, rnn_loss: 0.091\n",
      "Epoch: [28/200] [  50/ 115] time: 0.4708s, d_loss: 1.859, g_loss: 0.396, rnn_loss: 0.041\n",
      "Epoch: [28/200] [ 100/ 115] time: 0.4534s, d_loss: 1.350, g_loss: 0.775, rnn_loss: 0.082\n",
      "Epoch: [29/200] [   0/ 115] time: 0.4583s, d_loss: 1.398, g_loss: 0.961, rnn_loss: 0.082\n",
      "Epoch: [29/200] [  50/ 115] time: 0.4669s, d_loss: 1.440, g_loss: 0.834, rnn_loss: 0.054\n",
      "Epoch: [29/200] [ 100/ 115] time: 0.4679s, d_loss: 1.378, g_loss: 1.026, rnn_loss: 0.049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [30/200] [   0/ 115] time: 0.4857s, d_loss: 1.302, g_loss: 1.248, rnn_loss: 0.048\n",
      "Epoch: [30/200] [  50/ 115] time: 0.4733s, d_loss: 1.787, g_loss: 0.589, rnn_loss: 0.033\n",
      "Epoch: [30/200] [ 100/ 115] time: 0.4550s, d_loss: 1.295, g_loss: 1.292, rnn_loss: 0.045\n",
      "Epoch: [31/200] [   0/ 115] time: 0.4540s, d_loss: 1.369, g_loss: 1.097, rnn_loss: 0.061\n",
      "Epoch: [31/200] [  50/ 115] time: 0.4739s, d_loss: 1.411, g_loss: 0.810, rnn_loss: 0.066\n",
      "Epoch: [31/200] [ 100/ 115] time: 0.4529s, d_loss: 1.384, g_loss: 0.881, rnn_loss: 0.063\n",
      "Epoch: [32/200] [   0/ 115] time: 0.4520s, d_loss: 1.373, g_loss: 1.124, rnn_loss: 0.052\n",
      "Epoch: [32/200] [  50/ 115] time: 0.4679s, d_loss: 1.295, g_loss: 1.301, rnn_loss: 0.067\n",
      "Epoch: [32/200] [ 100/ 115] time: 0.4547s, d_loss: 1.322, g_loss: 1.175, rnn_loss: 0.031\n",
      "Epoch: [33/200] [   0/ 115] time: 0.4499s, d_loss: 1.346, g_loss: 1.267, rnn_loss: 0.036\n",
      "Epoch: [33/200] [  50/ 115] time: 0.4651s, d_loss: 1.391, g_loss: 1.005, rnn_loss: 0.059\n",
      "Epoch: [33/200] [ 100/ 115] time: 0.4556s, d_loss: 1.492, g_loss: 0.514, rnn_loss: 0.042\n",
      "Epoch: [34/200] [   0/ 115] time: 0.4478s, d_loss: 1.432, g_loss: 0.878, rnn_loss: 0.062\n",
      "Epoch: [34/200] [  50/ 115] time: 0.4585s, d_loss: 1.300, g_loss: 1.304, rnn_loss: 0.062\n",
      "Epoch: [34/200] [ 100/ 115] time: 0.4548s, d_loss: 1.323, g_loss: 1.690, rnn_loss: 0.039\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [35/200] [   0/ 115] time: 0.4857s, d_loss: 1.342, g_loss: 1.114, rnn_loss: 0.050\n",
      "Epoch: [35/200] [  50/ 115] time: 0.4639s, d_loss: 1.364, g_loss: 1.282, rnn_loss: 0.085\n",
      "Epoch: [35/200] [ 100/ 115] time: 0.4618s, d_loss: 1.316, g_loss: 1.619, rnn_loss: 0.030\n",
      "Epoch: [36/200] [   0/ 115] time: 0.4489s, d_loss: 1.334, g_loss: 1.168, rnn_loss: 0.040\n",
      "Epoch: [36/200] [  50/ 115] time: 0.4618s, d_loss: 1.322, g_loss: 1.609, rnn_loss: 0.059\n",
      "Epoch: [36/200] [ 100/ 115] time: 0.4549s, d_loss: 1.343, g_loss: 1.757, rnn_loss: 0.057\n",
      "Epoch: [37/200] [   0/ 115] time: 0.4458s, d_loss: 1.375, g_loss: 0.861, rnn_loss: 0.028\n",
      "Epoch: [37/200] [  50/ 115] time: 0.4549s, d_loss: 1.260, g_loss: 1.753, rnn_loss: 0.048\n",
      "Epoch: [37/200] [ 100/ 115] time: 0.4557s, d_loss: 1.299, g_loss: 1.348, rnn_loss: 0.051\n",
      "Epoch: [38/200] [   0/ 115] time: 0.4508s, d_loss: 1.372, g_loss: 0.984, rnn_loss: 0.036\n",
      "Epoch: [38/200] [  50/ 115] time: 0.4490s, d_loss: 1.365, g_loss: 1.008, rnn_loss: 0.037\n",
      "Epoch: [38/200] [ 100/ 115] time: 0.4593s, d_loss: 1.355, g_loss: 1.220, rnn_loss: 0.050\n",
      "Epoch: [39/200] [   0/ 115] time: 0.4568s, d_loss: 1.376, g_loss: 1.100, rnn_loss: 0.026\n",
      "Epoch: [39/200] [  50/ 115] time: 0.4589s, d_loss: 1.552, g_loss: 0.526, rnn_loss: 0.049\n",
      "Epoch: [39/200] [ 100/ 115] time: 0.4589s, d_loss: 1.361, g_loss: 1.303, rnn_loss: 0.032\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [40/200] [   0/ 115] time: 0.4691s, d_loss: 1.351, g_loss: 1.107, rnn_loss: 0.040\n",
      "Epoch: [40/200] [  50/ 115] time: 0.4447s, d_loss: 1.357, g_loss: 1.243, rnn_loss: 0.038\n",
      "Epoch: [40/200] [ 100/ 115] time: 0.4661s, d_loss: 1.304, g_loss: 1.157, rnn_loss: 0.027\n",
      "Epoch: [41/200] [   0/ 115] time: 0.4459s, d_loss: 1.434, g_loss: 0.747, rnn_loss: 0.054\n",
      "Epoch: [41/200] [  50/ 115] time: 0.4588s, d_loss: 1.459, g_loss: 1.011, rnn_loss: 0.043\n",
      "Epoch: [41/200] [ 100/ 115] time: 0.4619s, d_loss: 1.368, g_loss: 1.070, rnn_loss: 0.039\n",
      "Epoch: [42/200] [   0/ 115] time: 0.4525s, d_loss: 1.319, g_loss: 1.573, rnn_loss: 0.056\n",
      "Epoch: [42/200] [  50/ 115] time: 0.4459s, d_loss: 1.308, g_loss: 1.668, rnn_loss: 0.062\n",
      "Epoch: [42/200] [ 100/ 115] time: 0.4649s, d_loss: 1.326, g_loss: 1.241, rnn_loss: 0.053\n",
      "Epoch: [43/200] [   0/ 115] time: 0.4536s, d_loss: 1.275, g_loss: 1.647, rnn_loss: 0.039\n",
      "Epoch: [43/200] [  50/ 115] time: 0.4390s, d_loss: 1.429, g_loss: 0.599, rnn_loss: 0.046\n",
      "Epoch: [43/200] [ 100/ 115] time: 0.4538s, d_loss: 1.357, g_loss: 1.020, rnn_loss: 0.032\n",
      "Epoch: [44/200] [   0/ 115] time: 0.4499s, d_loss: 1.363, g_loss: 1.053, rnn_loss: 0.037\n",
      "Epoch: [44/200] [  50/ 115] time: 0.4408s, d_loss: 1.363, g_loss: 1.171, rnn_loss: 0.049\n",
      "Epoch: [44/200] [ 100/ 115] time: 0.4489s, d_loss: 1.312, g_loss: 1.241, rnn_loss: 0.023\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [45/200] [   0/ 115] time: 0.4859s, d_loss: 1.342, g_loss: 1.243, rnn_loss: 0.040\n",
      "Epoch: [45/200] [  50/ 115] time: 0.4448s, d_loss: 1.354, g_loss: 1.157, rnn_loss: 0.046\n",
      "Epoch: [45/200] [ 100/ 115] time: 0.4506s, d_loss: 1.369, g_loss: 0.946, rnn_loss: 0.026\n",
      "Epoch: [46/200] [   0/ 115] time: 0.4535s, d_loss: 1.406, g_loss: 0.959, rnn_loss: 0.034\n",
      "Epoch: [46/200] [  50/ 115] time: 0.4399s, d_loss: 1.398, g_loss: 1.936, rnn_loss: 0.043\n",
      "Epoch: [46/200] [ 100/ 115] time: 0.4549s, d_loss: 1.344, g_loss: 1.109, rnn_loss: 0.023\n",
      "Epoch: [47/200] [   0/ 115] time: 0.4572s, d_loss: 1.400, g_loss: 0.999, rnn_loss: 0.035\n",
      "Epoch: [47/200] [  50/ 115] time: 0.4458s, d_loss: 1.300, g_loss: 1.650, rnn_loss: 0.044\n",
      "Epoch: [47/200] [ 100/ 115] time: 0.4532s, d_loss: 1.339, g_loss: 0.931, rnn_loss: 0.028\n",
      "Epoch: [48/200] [   0/ 115] time: 0.4550s, d_loss: 1.356, g_loss: 1.076, rnn_loss: 0.023\n",
      "Epoch: [48/200] [  50/ 115] time: 0.4435s, d_loss: 1.253, g_loss: 1.523, rnn_loss: 0.045\n",
      "Epoch: [48/200] [ 100/ 115] time: 0.4519s, d_loss: 1.372, g_loss: 0.937, rnn_loss: 0.029\n",
      "Epoch: [49/200] [   0/ 115] time: 0.4449s, d_loss: 1.437, g_loss: 0.784, rnn_loss: 0.033\n",
      "Epoch: [49/200] [  50/ 115] time: 0.4500s, d_loss: 1.325, g_loss: 1.053, rnn_loss: 0.047\n",
      "Epoch: [49/200] [ 100/ 115] time: 0.4497s, d_loss: 1.300, g_loss: 1.310, rnn_loss: 0.033\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [50/200] [   0/ 115] time: 0.4881s, d_loss: 1.332, g_loss: 1.290, rnn_loss: 0.036\n",
      "Epoch: [50/200] [  50/ 115] time: 0.4530s, d_loss: 1.259, g_loss: 1.393, rnn_loss: 0.025\n",
      "Epoch: [50/200] [ 100/ 115] time: 0.4508s, d_loss: 1.415, g_loss: 1.044, rnn_loss: 0.026\n",
      "Epoch: [51/200] [   0/ 115] time: 0.4513s, d_loss: 1.356, g_loss: 1.094, rnn_loss: 0.032\n",
      "Epoch: [51/200] [  50/ 115] time: 0.4549s, d_loss: 1.354, g_loss: 1.128, rnn_loss: 0.044\n",
      "Epoch: [51/200] [ 100/ 115] time: 0.4580s, d_loss: 1.322, g_loss: 1.012, rnn_loss: 0.016\n",
      "Epoch: [52/200] [   0/ 115] time: 0.4490s, d_loss: 1.404, g_loss: 1.009, rnn_loss: 0.033\n",
      "Epoch: [52/200] [  50/ 115] time: 0.4593s, d_loss: 1.383, g_loss: 1.009, rnn_loss: 0.058\n",
      "Epoch: [52/200] [ 100/ 115] time: 0.4529s, d_loss: 1.398, g_loss: 0.911, rnn_loss: 0.016\n",
      "Epoch: [53/200] [   0/ 115] time: 0.4498s, d_loss: 1.555, g_loss: 0.707, rnn_loss: 0.046\n",
      "Epoch: [53/200] [  50/ 115] time: 0.4634s, d_loss: 1.361, g_loss: 1.052, rnn_loss: 0.033\n",
      "Epoch: [53/200] [ 100/ 115] time: 0.4598s, d_loss: 1.312, g_loss: 1.150, rnn_loss: 0.037\n",
      "Epoch: [54/200] [   0/ 115] time: 0.4498s, d_loss: 1.432, g_loss: 0.869, rnn_loss: 0.028\n",
      "Epoch: [54/200] [  50/ 115] time: 0.4649s, d_loss: 1.275, g_loss: 1.438, rnn_loss: 0.057\n",
      "Epoch: [54/200] [ 100/ 115] time: 0.4639s, d_loss: 1.344, g_loss: 1.143, rnn_loss: 0.026\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [55/200] [   0/ 115] time: 0.4776s, d_loss: 1.395, g_loss: 0.910, rnn_loss: 0.025\n",
      "Epoch: [55/200] [  50/ 115] time: 0.4738s, d_loss: 1.403, g_loss: 0.988, rnn_loss: 0.036\n",
      "Epoch: [55/200] [ 100/ 115] time: 0.4578s, d_loss: 1.295, g_loss: 1.361, rnn_loss: 0.045\n",
      "Epoch: [56/200] [   0/ 115] time: 0.4498s, d_loss: 1.324, g_loss: 1.333, rnn_loss: 0.066\n",
      "Epoch: [56/200] [  50/ 115] time: 0.4734s, d_loss: 1.272, g_loss: 1.122, rnn_loss: 0.036\n",
      "Epoch: [56/200] [ 100/ 115] time: 0.4589s, d_loss: 1.334, g_loss: 1.053, rnn_loss: 0.017\n",
      "Epoch: [57/200] [   0/ 115] time: 0.4548s, d_loss: 1.355, g_loss: 1.005, rnn_loss: 0.037\n",
      "Epoch: [57/200] [  50/ 115] time: 0.4819s, d_loss: 1.277, g_loss: 1.657, rnn_loss: 0.026\n",
      "Epoch: [57/200] [ 100/ 115] time: 0.4576s, d_loss: 1.331, g_loss: 1.206, rnn_loss: 0.036\n",
      "Epoch: [58/200] [   0/ 115] time: 0.4619s, d_loss: 1.374, g_loss: 1.024, rnn_loss: 0.015\n",
      "Epoch: [58/200] [  50/ 115] time: 0.4823s, d_loss: 1.284, g_loss: 1.463, rnn_loss: 0.016\n",
      "Epoch: [58/200] [ 100/ 115] time: 0.4568s, d_loss: 1.251, g_loss: 1.402, rnn_loss: 0.031\n",
      "Epoch: [59/200] [   0/ 115] time: 0.4550s, d_loss: 1.322, g_loss: 1.342, rnn_loss: 0.028\n",
      "Epoch: [59/200] [  50/ 115] time: 0.4848s, d_loss: 1.379, g_loss: 1.185, rnn_loss: 0.018\n",
      "Epoch: [59/200] [ 100/ 115] time: 0.4577s, d_loss: 1.312, g_loss: 1.334, rnn_loss: 0.038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [60/200] [   0/ 115] time: 0.4899s, d_loss: 1.334, g_loss: 1.372, rnn_loss: 0.030\n",
      "Epoch: [60/200] [  50/ 115] time: 0.4821s, d_loss: 1.324, g_loss: 1.695, rnn_loss: 0.023\n",
      "Epoch: [60/200] [ 100/ 115] time: 0.4697s, d_loss: 1.294, g_loss: 1.202, rnn_loss: 0.032\n",
      "Epoch: [61/200] [   0/ 115] time: 0.4604s, d_loss: 1.382, g_loss: 1.172, rnn_loss: 0.030\n",
      "Epoch: [61/200] [  50/ 115] time: 0.4789s, d_loss: 1.354, g_loss: 1.140, rnn_loss: 0.048\n",
      "Epoch: [61/200] [ 100/ 115] time: 0.4543s, d_loss: 1.382, g_loss: 1.099, rnn_loss: 0.018\n",
      "Epoch: [62/200] [   0/ 115] time: 0.4571s, d_loss: 1.272, g_loss: 1.503, rnn_loss: 0.016\n",
      "Epoch: [62/200] [  50/ 115] time: 0.4789s, d_loss: 1.309, g_loss: 1.681, rnn_loss: 0.023\n",
      "Epoch: [62/200] [ 100/ 115] time: 0.4569s, d_loss: 1.396, g_loss: 1.177, rnn_loss: 0.025\n",
      "Epoch: [63/200] [   0/ 115] time: 0.4638s, d_loss: 1.389, g_loss: 1.005, rnn_loss: 0.015\n",
      "Epoch: [63/200] [  50/ 115] time: 0.4783s, d_loss: 1.231, g_loss: 1.669, rnn_loss: 0.042\n",
      "Epoch: [63/200] [ 100/ 115] time: 0.4677s, d_loss: 1.386, g_loss: 1.120, rnn_loss: 0.020\n",
      "Epoch: [64/200] [   0/ 115] time: 0.4618s, d_loss: 1.454, g_loss: 0.987, rnn_loss: 0.010\n",
      "Epoch: [64/200] [  50/ 115] time: 0.4853s, d_loss: 1.250, g_loss: 1.872, rnn_loss: 0.034\n",
      "Epoch: [64/200] [ 100/ 115] time: 0.4553s, d_loss: 1.368, g_loss: 1.549, rnn_loss: 0.019\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [65/200] [   0/ 115] time: 0.4932s, d_loss: 1.386, g_loss: 1.063, rnn_loss: 0.029\n",
      "Epoch: [65/200] [  50/ 115] time: 0.4821s, d_loss: 1.296, g_loss: 1.445, rnn_loss: 0.048\n",
      "Epoch: [65/200] [ 100/ 115] time: 0.4587s, d_loss: 1.330, g_loss: 1.358, rnn_loss: 0.030\n",
      "Epoch: [66/200] [   0/ 115] time: 0.4638s, d_loss: 1.430, g_loss: 1.832, rnn_loss: 0.025\n",
      "Epoch: [66/200] [  50/ 115] time: 0.4827s, d_loss: 1.396, g_loss: 1.406, rnn_loss: 0.047\n",
      "Epoch: [66/200] [ 100/ 115] time: 0.4654s, d_loss: 1.354, g_loss: 1.054, rnn_loss: 0.045\n",
      "Epoch: [67/200] [   0/ 115] time: 0.4618s, d_loss: 1.351, g_loss: 1.332, rnn_loss: 0.039\n",
      "Epoch: [67/200] [  50/ 115] time: 0.4794s, d_loss: 1.349, g_loss: 1.073, rnn_loss: 0.023\n",
      "Epoch: [67/200] [ 100/ 115] time: 0.4609s, d_loss: 1.364, g_loss: 1.064, rnn_loss: 0.031\n",
      "Epoch: [68/200] [   0/ 115] time: 0.4680s, d_loss: 1.378, g_loss: 1.221, rnn_loss: 0.038\n",
      "Epoch: [68/200] [  50/ 115] time: 0.4777s, d_loss: 1.225, g_loss: 1.540, rnn_loss: 0.013\n",
      "Epoch: [68/200] [ 100/ 115] time: 0.4545s, d_loss: 1.382, g_loss: 0.923, rnn_loss: 0.035\n",
      "Epoch: [69/200] [   0/ 115] time: 0.4759s, d_loss: 1.452, g_loss: 1.072, rnn_loss: 0.020\n",
      "Epoch: [69/200] [  50/ 115] time: 0.4968s, d_loss: 1.337, g_loss: 1.134, rnn_loss: 0.042\n",
      "Epoch: [69/200] [ 100/ 115] time: 0.4729s, d_loss: 1.304, g_loss: 1.347, rnn_loss: 0.024\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [70/200] [   0/ 115] time: 0.4977s, d_loss: 1.473, g_loss: 0.793, rnn_loss: 0.043\n",
      "Epoch: [70/200] [  50/ 115] time: 0.4768s, d_loss: 1.376, g_loss: 1.268, rnn_loss: 0.027\n",
      "Epoch: [70/200] [ 100/ 115] time: 0.4705s, d_loss: 1.373, g_loss: 0.957, rnn_loss: 0.027\n",
      "Epoch: [71/200] [   0/ 115] time: 0.4638s, d_loss: 1.456, g_loss: 0.940, rnn_loss: 0.032\n",
      "Epoch: [71/200] [  50/ 115] time: 0.4742s, d_loss: 1.341, g_loss: 1.300, rnn_loss: 0.019\n",
      "Epoch: [71/200] [ 100/ 115] time: 0.4645s, d_loss: 1.350, g_loss: 1.282, rnn_loss: 0.012\n",
      "Epoch: [72/200] [   0/ 115] time: 0.4688s, d_loss: 1.459, g_loss: 0.853, rnn_loss: 0.019\n",
      "Epoch: [72/200] [  50/ 115] time: 0.4688s, d_loss: 1.388, g_loss: 1.059, rnn_loss: 0.012\n",
      "Epoch: [72/200] [ 100/ 115] time: 0.4673s, d_loss: 1.307, g_loss: 1.122, rnn_loss: 0.028\n",
      "Epoch: [73/200] [   0/ 115] time: 0.4598s, d_loss: 1.385, g_loss: 1.087, rnn_loss: 0.048\n",
      "Epoch: [73/200] [  50/ 115] time: 0.4618s, d_loss: 1.292, g_loss: 1.525, rnn_loss: 0.024\n",
      "Epoch: [73/200] [ 100/ 115] time: 0.4560s, d_loss: 1.385, g_loss: 1.084, rnn_loss: 0.032\n",
      "Epoch: [74/200] [   0/ 115] time: 0.4583s, d_loss: 1.345, g_loss: 1.313, rnn_loss: 0.029\n",
      "Epoch: [74/200] [  50/ 115] time: 0.4618s, d_loss: 1.339, g_loss: 1.225, rnn_loss: 0.018\n",
      "Epoch: [74/200] [ 100/ 115] time: 0.4635s, d_loss: 1.382, g_loss: 1.272, rnn_loss: 0.033\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [75/200] [   0/ 115] time: 0.4837s, d_loss: 1.410, g_loss: 0.932, rnn_loss: 0.029\n",
      "Epoch: [75/200] [  50/ 115] time: 0.4571s, d_loss: 1.382, g_loss: 0.983, rnn_loss: 0.023\n",
      "Epoch: [75/200] [ 100/ 115] time: 0.4631s, d_loss: 1.331, g_loss: 1.083, rnn_loss: 0.019\n",
      "Epoch: [76/200] [   0/ 115] time: 0.4578s, d_loss: 1.778, g_loss: 0.640, rnn_loss: 0.037\n",
      "Epoch: [76/200] [  50/ 115] time: 0.4539s, d_loss: 1.313, g_loss: 1.166, rnn_loss: 0.026\n",
      "Epoch: [76/200] [ 100/ 115] time: 0.4526s, d_loss: 1.277, g_loss: 1.365, rnn_loss: 0.023\n",
      "Epoch: [77/200] [   0/ 115] time: 0.4540s, d_loss: 1.346, g_loss: 0.946, rnn_loss: 0.008\n",
      "Epoch: [77/200] [  50/ 115] time: 0.4601s, d_loss: 1.405, g_loss: 1.082, rnn_loss: 0.030\n",
      "Epoch: [77/200] [ 100/ 115] time: 0.4495s, d_loss: 1.328, g_loss: 1.281, rnn_loss: 0.030\n",
      "Epoch: [78/200] [   0/ 115] time: 0.4568s, d_loss: 1.498, g_loss: 1.275, rnn_loss: 0.036\n",
      "Epoch: [78/200] [  50/ 115] time: 0.4569s, d_loss: 1.340, g_loss: 0.989, rnn_loss: 0.023\n",
      "Epoch: [78/200] [ 100/ 115] time: 0.4479s, d_loss: 1.329, g_loss: 1.109, rnn_loss: 0.036\n",
      "Epoch: [79/200] [   0/ 115] time: 0.4550s, d_loss: 1.505, g_loss: 0.907, rnn_loss: 0.023\n",
      "Epoch: [79/200] [  50/ 115] time: 0.4629s, d_loss: 1.416, g_loss: 1.028, rnn_loss: 0.024\n",
      "Epoch: [79/200] [ 100/ 115] time: 0.4470s, d_loss: 1.314, g_loss: 1.210, rnn_loss: 0.014\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [80/200] [   0/ 115] time: 0.4781s, d_loss: 1.410, g_loss: 0.857, rnn_loss: 0.009\n",
      "Epoch: [80/200] [  50/ 115] time: 0.4633s, d_loss: 1.400, g_loss: 1.052, rnn_loss: 0.020\n",
      "Epoch: [80/200] [ 100/ 115] time: 0.4469s, d_loss: 1.356, g_loss: 1.159, rnn_loss: 0.029\n",
      "Epoch: [81/200] [   0/ 115] time: 0.4488s, d_loss: 1.437, g_loss: 1.040, rnn_loss: 0.026\n",
      "Epoch: [81/200] [  50/ 115] time: 0.4618s, d_loss: 1.300, g_loss: 1.265, rnn_loss: 0.020\n",
      "Epoch: [81/200] [ 100/ 115] time: 0.4518s, d_loss: 1.299, g_loss: 1.404, rnn_loss: 0.013\n",
      "Epoch: [82/200] [   0/ 115] time: 0.4530s, d_loss: 1.446, g_loss: 0.993, rnn_loss: 0.019\n",
      "Epoch: [82/200] [  50/ 115] time: 0.4625s, d_loss: 1.351, g_loss: 1.186, rnn_loss: 0.013\n",
      "Epoch: [82/200] [ 100/ 115] time: 0.4764s, d_loss: 1.335, g_loss: 1.139, rnn_loss: 0.035\n",
      "Epoch: [83/200] [   0/ 115] time: 0.4341s, d_loss: 1.645, g_loss: 0.537, rnn_loss: 0.021\n",
      "Epoch: [83/200] [  50/ 115] time: 0.4488s, d_loss: 1.259, g_loss: 1.621, rnn_loss: 0.037\n",
      "Epoch: [83/200] [ 100/ 115] time: 0.4369s, d_loss: 1.303, g_loss: 1.477, rnn_loss: 0.035\n",
      "Epoch: [84/200] [   0/ 115] time: 0.4321s, d_loss: 1.437, g_loss: 1.030, rnn_loss: 0.024\n",
      "Epoch: [84/200] [  50/ 115] time: 0.4549s, d_loss: 1.260, g_loss: 1.644, rnn_loss: 0.033\n",
      "Epoch: [84/200] [ 100/ 115] time: 0.4388s, d_loss: 1.321, g_loss: 1.232, rnn_loss: 0.044\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [85/200] [   0/ 115] time: 0.4551s, d_loss: 1.470, g_loss: 0.565, rnn_loss: 0.021\n",
      "Epoch: [85/200] [  50/ 115] time: 0.4450s, d_loss: 1.321, g_loss: 1.048, rnn_loss: 0.022\n",
      "Epoch: [85/200] [ 100/ 115] time: 0.4413s, d_loss: 1.272, g_loss: 1.788, rnn_loss: 0.021\n",
      "Epoch: [86/200] [   0/ 115] time: 0.4328s, d_loss: 1.418, g_loss: 1.757, rnn_loss: 0.017\n",
      "Epoch: [86/200] [  50/ 115] time: 0.4491s, d_loss: 1.293, g_loss: 1.238, rnn_loss: 0.012\n",
      "Epoch: [86/200] [ 100/ 115] time: 0.4394s, d_loss: 1.357, g_loss: 1.053, rnn_loss: 0.021\n",
      "Epoch: [87/200] [   0/ 115] time: 0.4348s, d_loss: 1.436, g_loss: 0.769, rnn_loss: 0.014\n",
      "Epoch: [87/200] [  50/ 115] time: 0.4512s, d_loss: 1.302, g_loss: 1.267, rnn_loss: 0.016\n",
      "Epoch: [87/200] [ 100/ 115] time: 0.4406s, d_loss: 1.432, g_loss: 1.215, rnn_loss: 0.015\n",
      "Epoch: [88/200] [   0/ 115] time: 0.4388s, d_loss: 1.469, g_loss: 0.672, rnn_loss: 0.025\n",
      "Epoch: [88/200] [  50/ 115] time: 0.4460s, d_loss: 1.279, g_loss: 1.251, rnn_loss: 0.019\n",
      "Epoch: [88/200] [ 100/ 115] time: 0.4461s, d_loss: 1.425, g_loss: 1.329, rnn_loss: 0.031\n",
      "Epoch: [89/200] [   0/ 115] time: 0.4348s, d_loss: 1.310, g_loss: 1.020, rnn_loss: 0.020\n",
      "Epoch: [89/200] [  50/ 115] time: 0.4478s, d_loss: 1.289, g_loss: 1.465, rnn_loss: 0.011\n",
      "Epoch: [89/200] [ 100/ 115] time: 0.4438s, d_loss: 1.286, g_loss: 1.126, rnn_loss: 0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [90/200] [   0/ 115] time: 0.4598s, d_loss: 1.337, g_loss: 1.239, rnn_loss: 0.017\n",
      "Epoch: [90/200] [  50/ 115] time: 0.4479s, d_loss: 1.353, g_loss: 1.000, rnn_loss: 0.020\n",
      "Epoch: [90/200] [ 100/ 115] time: 0.4484s, d_loss: 1.308, g_loss: 1.446, rnn_loss: 0.032\n",
      "Epoch: [91/200] [   0/ 115] time: 0.4377s, d_loss: 1.389, g_loss: 1.037, rnn_loss: 0.015\n",
      "Epoch: [91/200] [  50/ 115] time: 0.4524s, d_loss: 1.391, g_loss: 1.277, rnn_loss: 0.022\n",
      "Epoch: [91/200] [ 100/ 115] time: 0.4472s, d_loss: 1.360, g_loss: 0.979, rnn_loss: 0.017\n",
      "Epoch: [92/200] [   0/ 115] time: 0.4388s, d_loss: 1.330, g_loss: 1.335, rnn_loss: 0.027\n",
      "Epoch: [92/200] [  50/ 115] time: 0.4498s, d_loss: 1.302, g_loss: 1.205, rnn_loss: 0.026\n",
      "Epoch: [92/200] [ 100/ 115] time: 0.4501s, d_loss: 1.335, g_loss: 1.189, rnn_loss: 0.026\n",
      "Epoch: [93/200] [   0/ 115] time: 0.4370s, d_loss: 1.349, g_loss: 1.272, rnn_loss: 0.019\n",
      "Epoch: [93/200] [  50/ 115] time: 0.4591s, d_loss: 1.306, g_loss: 1.294, rnn_loss: 0.024\n",
      "Epoch: [93/200] [ 100/ 115] time: 0.4479s, d_loss: 1.370, g_loss: 1.267, rnn_loss: 0.019\n",
      "Epoch: [94/200] [   0/ 115] time: 0.4421s, d_loss: 1.280, g_loss: 1.280, rnn_loss: 0.026\n",
      "Epoch: [94/200] [  50/ 115] time: 0.4500s, d_loss: 1.316, g_loss: 1.311, rnn_loss: 0.020\n",
      "Epoch: [94/200] [ 100/ 115] time: 0.4485s, d_loss: 1.325, g_loss: 1.096, rnn_loss: 0.026\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [95/200] [   0/ 115] time: 0.4709s, d_loss: 1.300, g_loss: 1.054, rnn_loss: 0.021\n",
      "Epoch: [95/200] [  50/ 115] time: 0.4472s, d_loss: 1.360, g_loss: 1.018, rnn_loss: 0.012\n",
      "Epoch: [95/200] [ 100/ 115] time: 0.4458s, d_loss: 1.303, g_loss: 1.377, rnn_loss: 0.024\n",
      "Epoch: [96/200] [   0/ 115] time: 0.4437s, d_loss: 1.313, g_loss: 1.731, rnn_loss: 0.046\n",
      "Epoch: [96/200] [  50/ 115] time: 0.4385s, d_loss: 1.285, g_loss: 1.255, rnn_loss: 0.029\n",
      "Epoch: [96/200] [ 100/ 115] time: 0.4441s, d_loss: 1.525, g_loss: 0.972, rnn_loss: 0.006\n",
      "Epoch: [97/200] [   0/ 115] time: 0.4400s, d_loss: 1.227, g_loss: 1.626, rnn_loss: 0.010\n",
      "Epoch: [97/200] [  50/ 115] time: 0.4418s, d_loss: 1.259, g_loss: 1.550, rnn_loss: 0.019\n",
      "Epoch: [97/200] [ 100/ 115] time: 0.4422s, d_loss: 1.413, g_loss: 1.136, rnn_loss: 0.004\n",
      "Epoch: [98/200] [   0/ 115] time: 0.4390s, d_loss: 1.348, g_loss: 1.709, rnn_loss: 0.019\n",
      "Epoch: [98/200] [  50/ 115] time: 0.4385s, d_loss: 1.292, g_loss: 1.481, rnn_loss: 0.014\n",
      "Epoch: [98/200] [ 100/ 115] time: 0.4443s, d_loss: 1.333, g_loss: 1.346, rnn_loss: 0.019\n",
      "Epoch: [99/200] [   0/ 115] time: 0.4474s, d_loss: 1.218, g_loss: 1.216, rnn_loss: 0.015\n",
      "Epoch: [99/200] [  50/ 115] time: 0.4433s, d_loss: 1.238, g_loss: 1.694, rnn_loss: 0.039\n",
      "Epoch: [99/200] [ 100/ 115] time: 0.4406s, d_loss: 1.385, g_loss: 1.065, rnn_loss: 0.031\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [100/200] [   0/ 115] time: 0.4628s, d_loss: 1.190, g_loss: 1.620, rnn_loss: 0.027\n",
      "Epoch: [100/200] [  50/ 115] time: 0.4350s, d_loss: 1.284, g_loss: 1.232, rnn_loss: 0.015\n",
      "Epoch: [100/200] [ 100/ 115] time: 0.4394s, d_loss: 1.342, g_loss: 1.282, rnn_loss: 0.025\n",
      "Epoch: [101/200] [   0/ 115] time: 0.4518s, d_loss: 1.174, g_loss: 1.429, rnn_loss: 0.010\n",
      "Epoch: [101/200] [  50/ 115] time: 0.4394s, d_loss: 1.236, g_loss: 1.538, rnn_loss: 0.013\n",
      "Epoch: [101/200] [ 100/ 115] time: 0.4493s, d_loss: 1.253, g_loss: 1.179, rnn_loss: 0.011\n",
      "Epoch: [102/200] [   0/ 115] time: 0.4429s, d_loss: 1.229, g_loss: 1.485, rnn_loss: 0.015\n",
      "Epoch: [102/200] [  50/ 115] time: 0.4423s, d_loss: 1.311, g_loss: 1.329, rnn_loss: 0.007\n",
      "Epoch: [102/200] [ 100/ 115] time: 0.4498s, d_loss: 1.294, g_loss: 1.191, rnn_loss: 0.032\n",
      "Epoch: [103/200] [   0/ 115] time: 0.4435s, d_loss: 1.134, g_loss: 2.058, rnn_loss: 0.013\n",
      "Epoch: [103/200] [  50/ 115] time: 0.4382s, d_loss: 1.290, g_loss: 1.349, rnn_loss: 0.013\n",
      "Epoch: [103/200] [ 100/ 115] time: 0.4448s, d_loss: 1.205, g_loss: 1.784, rnn_loss: 0.013\n",
      "Epoch: [104/200] [   0/ 115] time: 0.4489s, d_loss: 1.142, g_loss: 1.746, rnn_loss: 0.008\n",
      "Epoch: [104/200] [  50/ 115] time: 0.4393s, d_loss: 1.268, g_loss: 1.204, rnn_loss: 0.020\n",
      "Epoch: [104/200] [ 100/ 115] time: 0.4469s, d_loss: 1.187, g_loss: 1.920, rnn_loss: 0.011\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [105/200] [   0/ 115] time: 0.4774s, d_loss: 1.225, g_loss: 1.663, rnn_loss: 0.018\n",
      "Epoch: [105/200] [  50/ 115] time: 0.4479s, d_loss: 1.254, g_loss: 1.561, rnn_loss: 0.019\n",
      "Epoch: [105/200] [ 100/ 115] time: 0.4476s, d_loss: 1.214, g_loss: 1.712, rnn_loss: 0.014\n",
      "Epoch: [106/200] [   0/ 115] time: 0.4444s, d_loss: 1.236, g_loss: 1.364, rnn_loss: 0.016\n",
      "Epoch: [106/200] [  50/ 115] time: 0.4443s, d_loss: 1.375, g_loss: 1.056, rnn_loss: 0.000\n",
      "Epoch: [106/200] [ 100/ 115] time: 0.4509s, d_loss: 1.159, g_loss: 1.817, rnn_loss: 0.020\n",
      "Epoch: [107/200] [   0/ 115] time: 0.4493s, d_loss: 1.153, g_loss: 2.249, rnn_loss: 0.014\n",
      "Epoch: [107/200] [  50/ 115] time: 0.4439s, d_loss: 1.272, g_loss: 1.236, rnn_loss: 0.008\n",
      "Epoch: [107/200] [ 100/ 115] time: 0.4504s, d_loss: 1.210, g_loss: 1.568, rnn_loss: 0.019\n",
      "Epoch: [108/200] [   0/ 115] time: 0.4410s, d_loss: 1.260, g_loss: 1.320, rnn_loss: 0.013\n",
      "Epoch: [108/200] [  50/ 115] time: 0.4474s, d_loss: 1.186, g_loss: 1.553, rnn_loss: 0.009\n",
      "Epoch: [108/200] [ 100/ 115] time: 0.4578s, d_loss: 1.201, g_loss: 1.722, rnn_loss: 0.008\n",
      "Epoch: [109/200] [   0/ 115] time: 0.4351s, d_loss: 1.170, g_loss: 2.223, rnn_loss: 0.022\n",
      "Epoch: [109/200] [  50/ 115] time: 0.4668s, d_loss: 1.360, g_loss: 1.115, rnn_loss: 0.009\n",
      "Epoch: [109/200] [ 100/ 115] time: 0.5106s, d_loss: 1.186, g_loss: 2.016, rnn_loss: 0.026\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [110/200] [   0/ 115] time: 0.4800s, d_loss: 1.184, g_loss: 1.922, rnn_loss: 0.016\n",
      "Epoch: [110/200] [  50/ 115] time: 0.4669s, d_loss: 1.299, g_loss: 1.229, rnn_loss: 0.007\n",
      "Epoch: [110/200] [ 100/ 115] time: 0.4693s, d_loss: 1.173, g_loss: 1.685, rnn_loss: 0.020\n",
      "Epoch: [111/200] [   0/ 115] time: 0.4628s, d_loss: 1.261, g_loss: 1.517, rnn_loss: 0.009\n",
      "Epoch: [111/200] [  50/ 115] time: 0.4790s, d_loss: 1.225, g_loss: 1.584, rnn_loss: 0.013\n",
      "Epoch: [111/200] [ 100/ 115] time: 0.4653s, d_loss: 1.340, g_loss: 1.131, rnn_loss: 0.015\n",
      "Epoch: [112/200] [   0/ 115] time: 0.4579s, d_loss: 1.224, g_loss: 1.334, rnn_loss: 0.008\n",
      "Epoch: [112/200] [  50/ 115] time: 0.4873s, d_loss: 1.532, g_loss: 0.941, rnn_loss: 0.006\n",
      "Epoch: [112/200] [ 100/ 115] time: 0.4845s, d_loss: 1.204, g_loss: 1.545, rnn_loss: 0.007\n",
      "Epoch: [113/200] [   0/ 115] time: 0.4673s, d_loss: 1.200, g_loss: 1.560, rnn_loss: 0.022\n",
      "Epoch: [113/200] [  50/ 115] time: 0.4660s, d_loss: 1.216, g_loss: 1.705, rnn_loss: 0.011\n",
      "Epoch: [113/200] [ 100/ 115] time: 0.4795s, d_loss: 1.259, g_loss: 1.647, rnn_loss: 0.010\n",
      "Epoch: [114/200] [   0/ 115] time: 0.4550s, d_loss: 1.256, g_loss: 1.378, rnn_loss: 0.022\n",
      "Epoch: [114/200] [  50/ 115] time: 0.4708s, d_loss: 1.414, g_loss: 1.225, rnn_loss: 0.007\n",
      "Epoch: [114/200] [ 100/ 115] time: 0.4818s, d_loss: 1.256, g_loss: 1.684, rnn_loss: 0.014\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [115/200] [   0/ 115] time: 0.4722s, d_loss: 1.236, g_loss: 1.440, rnn_loss: 0.013\n",
      "Epoch: [115/200] [  50/ 115] time: 0.4648s, d_loss: 1.235, g_loss: 1.488, rnn_loss: 0.019\n",
      "Epoch: [115/200] [ 100/ 115] time: 0.4598s, d_loss: 1.243, g_loss: 1.594, rnn_loss: 0.012\n",
      "Epoch: [116/200] [   0/ 115] time: 0.4378s, d_loss: 1.269, g_loss: 0.857, rnn_loss: 0.011\n",
      "Epoch: [116/200] [  50/ 115] time: 0.4528s, d_loss: 1.342, g_loss: 1.280, rnn_loss: 0.013\n",
      "Epoch: [116/200] [ 100/ 115] time: 0.5825s, d_loss: 1.313, g_loss: 1.505, rnn_loss: 0.009\n",
      "Epoch: [117/200] [   0/ 115] time: 0.5454s, d_loss: 1.268, g_loss: 1.489, rnn_loss: 0.010\n",
      "Epoch: [117/200] [  50/ 115] time: 0.5256s, d_loss: 1.342, g_loss: 1.184, rnn_loss: 0.005\n",
      "Epoch: [117/200] [ 100/ 115] time: 0.5625s, d_loss: 1.471, g_loss: 0.830, rnn_loss: 0.010\n",
      "Epoch: [118/200] [   0/ 115] time: 0.5378s, d_loss: 1.149, g_loss: 1.589, rnn_loss: 0.013\n",
      "Epoch: [118/200] [  50/ 115] time: 0.5361s, d_loss: 1.352, g_loss: 1.178, rnn_loss: 0.014\n",
      "Epoch: [118/200] [ 100/ 115] time: 0.5501s, d_loss: 1.362, g_loss: 0.993, rnn_loss: 0.007\n",
      "Epoch: [119/200] [   0/ 115] time: 0.5306s, d_loss: 1.315, g_loss: 1.167, rnn_loss: 0.011\n",
      "Epoch: [119/200] [  50/ 115] time: 0.5376s, d_loss: 1.480, g_loss: 0.978, rnn_loss: 0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [119/200] [ 100/ 115] time: 0.5405s, d_loss: 1.291, g_loss: 1.404, rnn_loss: 0.017\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [120/200] [   0/ 115] time: 0.5620s, d_loss: 1.354, g_loss: 1.238, rnn_loss: 0.016\n",
      "Epoch: [120/200] [  50/ 115] time: 0.5603s, d_loss: 1.378, g_loss: 1.091, rnn_loss: 0.010\n",
      "Epoch: [120/200] [ 100/ 115] time: 0.5822s, d_loss: 1.421, g_loss: 0.697, rnn_loss: 0.025\n",
      "Epoch: [121/200] [   0/ 115] time: 0.5451s, d_loss: 1.233, g_loss: 1.366, rnn_loss: 0.020\n",
      "Epoch: [121/200] [  50/ 115] time: 0.5562s, d_loss: 1.183, g_loss: 1.411, rnn_loss: 0.016\n",
      "Epoch: [121/200] [ 100/ 115] time: 0.5496s, d_loss: 1.399, g_loss: 0.702, rnn_loss: 0.022\n",
      "Epoch: [122/200] [   0/ 115] time: 0.5290s, d_loss: 1.236, g_loss: 1.505, rnn_loss: 0.016\n",
      "Epoch: [122/200] [  50/ 115] time: 0.5264s, d_loss: 1.256, g_loss: 0.967, rnn_loss: 0.010\n",
      "Epoch: [122/200] [ 100/ 115] time: 0.5448s, d_loss: 1.369, g_loss: 1.029, rnn_loss: 0.007\n",
      "Epoch: [123/200] [   0/ 115] time: 0.5329s, d_loss: 1.275, g_loss: 1.408, rnn_loss: 0.016\n",
      "Epoch: [123/200] [  50/ 115] time: 0.5166s, d_loss: 1.286, g_loss: 1.221, rnn_loss: 0.005\n",
      "Epoch: [123/200] [ 100/ 115] time: 0.5436s, d_loss: 1.243, g_loss: 1.167, rnn_loss: 0.000\n",
      "Epoch: [124/200] [   0/ 115] time: 0.5412s, d_loss: 1.344, g_loss: 0.797, rnn_loss: 0.017\n",
      "Epoch: [124/200] [  50/ 115] time: 0.5331s, d_loss: 1.371, g_loss: 0.917, rnn_loss: 0.013\n",
      "Epoch: [124/200] [ 100/ 115] time: 0.5401s, d_loss: 1.317, g_loss: 1.164, rnn_loss: 0.017\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [125/200] [   0/ 115] time: 0.5517s, d_loss: 1.205, g_loss: 1.653, rnn_loss: 0.019\n",
      "Epoch: [125/200] [  50/ 115] time: 0.5253s, d_loss: 1.317, g_loss: 1.535, rnn_loss: 0.023\n",
      "Epoch: [125/200] [ 100/ 115] time: 0.5386s, d_loss: 1.349, g_loss: 0.931, rnn_loss: 0.009\n",
      "Epoch: [126/200] [   0/ 115] time: 0.5415s, d_loss: 1.257, g_loss: 1.877, rnn_loss: 0.010\n",
      "Epoch: [126/200] [  50/ 115] time: 0.5310s, d_loss: 1.319, g_loss: 1.320, rnn_loss: 0.006\n",
      "Epoch: [126/200] [ 100/ 115] time: 0.5567s, d_loss: 1.247, g_loss: 1.272, rnn_loss: 0.013\n",
      "Epoch: [127/200] [   0/ 115] time: 0.5381s, d_loss: 1.188, g_loss: 1.744, rnn_loss: 0.007\n",
      "Epoch: [127/200] [  50/ 115] time: 0.5266s, d_loss: 1.348, g_loss: 1.024, rnn_loss: 0.013\n",
      "Epoch: [127/200] [ 100/ 115] time: 0.5455s, d_loss: 1.347, g_loss: 0.832, rnn_loss: 0.007\n",
      "Epoch: [128/200] [   0/ 115] time: 0.5346s, d_loss: 1.180, g_loss: 1.428, rnn_loss: 0.025\n",
      "Epoch: [128/200] [  50/ 115] time: 0.5201s, d_loss: 1.274, g_loss: 1.322, rnn_loss: 0.014\n",
      "Epoch: [128/200] [ 100/ 115] time: 0.5386s, d_loss: 1.411, g_loss: 0.757, rnn_loss: 0.013\n",
      "Epoch: [129/200] [   0/ 115] time: 0.5430s, d_loss: 1.361, g_loss: 1.237, rnn_loss: 0.010\n",
      "Epoch: [129/200] [  50/ 115] time: 0.5163s, d_loss: 1.375, g_loss: 0.882, rnn_loss: 0.024\n",
      "Epoch: [129/200] [ 100/ 115] time: 0.5306s, d_loss: 1.443, g_loss: 0.812, rnn_loss: 0.021\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [130/200] [   0/ 115] time: 0.5661s, d_loss: 1.287, g_loss: 1.687, rnn_loss: 0.011\n",
      "Epoch: [130/200] [  50/ 115] time: 0.5411s, d_loss: 1.235, g_loss: 1.383, rnn_loss: 0.027\n",
      "Epoch: [130/200] [ 100/ 115] time: 0.5406s, d_loss: 1.372, g_loss: 0.914, rnn_loss: 0.009\n",
      "Epoch: [131/200] [   0/ 115] time: 0.5393s, d_loss: 1.278, g_loss: 1.836, rnn_loss: 0.013\n",
      "Epoch: [131/200] [  50/ 115] time: 0.5575s, d_loss: 1.380, g_loss: 1.077, rnn_loss: 0.017\n",
      "Epoch: [131/200] [ 100/ 115] time: 0.4638s, d_loss: 1.341, g_loss: 1.125, rnn_loss: 0.022\n",
      "Epoch: [132/200] [   0/ 115] time: 0.4518s, d_loss: 1.282, g_loss: 1.204, rnn_loss: 0.007\n",
      "Epoch: [132/200] [  50/ 115] time: 0.4391s, d_loss: 1.313, g_loss: 0.965, rnn_loss: 0.009\n",
      "Epoch: [132/200] [ 100/ 115] time: 0.4668s, d_loss: 1.498, g_loss: 0.970, rnn_loss: 0.024\n",
      "Epoch: [133/200] [   0/ 115] time: 0.4668s, d_loss: 1.279, g_loss: 1.279, rnn_loss: 0.012\n",
      "Epoch: [133/200] [  50/ 115] time: 0.4609s, d_loss: 1.487, g_loss: 0.726, rnn_loss: 0.017\n",
      "Epoch: [133/200] [ 100/ 115] time: 0.4680s, d_loss: 1.336, g_loss: 1.442, rnn_loss: 0.032\n",
      "Epoch: [134/200] [   0/ 115] time: 0.4617s, d_loss: 1.339, g_loss: 1.637, rnn_loss: 0.012\n",
      "Epoch: [134/200] [  50/ 115] time: 0.4648s, d_loss: 1.256, g_loss: 1.173, rnn_loss: 0.011\n",
      "Epoch: [134/200] [ 100/ 115] time: 0.4832s, d_loss: 1.339, g_loss: 1.233, rnn_loss: 0.005\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [135/200] [   0/ 115] time: 0.5042s, d_loss: 1.297, g_loss: 1.398, rnn_loss: 0.018\n",
      "Epoch: [135/200] [  50/ 115] time: 0.4907s, d_loss: 1.267, g_loss: 1.254, rnn_loss: 0.002\n",
      "Epoch: [135/200] [ 100/ 115] time: 0.4883s, d_loss: 1.528, g_loss: 0.956, rnn_loss: 0.013\n",
      "Epoch: [136/200] [   0/ 115] time: 0.4927s, d_loss: 1.207, g_loss: 1.330, rnn_loss: 0.019\n",
      "Epoch: [136/200] [  50/ 115] time: 0.4797s, d_loss: 1.243, g_loss: 1.523, rnn_loss: 0.020\n",
      "Epoch: [136/200] [ 100/ 115] time: 0.4658s, d_loss: 1.235, g_loss: 1.526, rnn_loss: 0.029\n",
      "Epoch: [137/200] [   0/ 115] time: 0.4664s, d_loss: 1.385, g_loss: 0.839, rnn_loss: 0.014\n",
      "Epoch: [137/200] [  50/ 115] time: 0.4630s, d_loss: 1.305, g_loss: 1.248, rnn_loss: 0.017\n",
      "Epoch: [137/200] [ 100/ 115] time: 0.4610s, d_loss: 1.371, g_loss: 1.171, rnn_loss: 0.011\n",
      "Epoch: [138/200] [   0/ 115] time: 0.4605s, d_loss: 1.346, g_loss: 0.771, rnn_loss: 0.017\n",
      "Epoch: [138/200] [  50/ 115] time: 0.4483s, d_loss: 1.219, g_loss: 1.459, rnn_loss: 0.024\n",
      "Epoch: [138/200] [ 100/ 115] time: 0.4638s, d_loss: 1.402, g_loss: 1.115, rnn_loss: 0.007\n",
      "Epoch: [139/200] [   0/ 115] time: 0.4556s, d_loss: 1.311, g_loss: 0.808, rnn_loss: 0.011\n",
      "Epoch: [139/200] [  50/ 115] time: 0.4388s, d_loss: 1.510, g_loss: 0.871, rnn_loss: 0.027\n",
      "Epoch: [139/200] [ 100/ 115] time: 0.4388s, d_loss: 1.192, g_loss: 1.691, rnn_loss: 0.005\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [140/200] [   0/ 115] time: 0.4697s, d_loss: 1.348, g_loss: 1.057, rnn_loss: 0.009\n",
      "Epoch: [140/200] [  50/ 115] time: 0.4393s, d_loss: 1.220, g_loss: 1.579, rnn_loss: 0.027\n",
      "Epoch: [140/200] [ 100/ 115] time: 0.4538s, d_loss: 1.153, g_loss: 1.441, rnn_loss: 0.009\n",
      "Epoch: [141/200] [   0/ 115] time: 0.4598s, d_loss: 1.405, g_loss: 1.105, rnn_loss: 0.010\n",
      "Epoch: [141/200] [  50/ 115] time: 0.4458s, d_loss: 1.187, g_loss: 1.697, rnn_loss: 0.007\n",
      "Epoch: [141/200] [ 100/ 115] time: 0.4498s, d_loss: 1.122, g_loss: 1.848, rnn_loss: 0.007\n",
      "Epoch: [142/200] [   0/ 115] time: 0.4508s, d_loss: 1.310, g_loss: 1.062, rnn_loss: 0.019\n",
      "Epoch: [142/200] [  50/ 115] time: 0.4587s, d_loss: 1.226, g_loss: 1.601, rnn_loss: 0.015\n",
      "Epoch: [142/200] [ 100/ 115] time: 0.5546s, d_loss: 1.277, g_loss: 1.505, rnn_loss: 0.011\n",
      "Epoch: [143/200] [   0/ 115] time: 0.5451s, d_loss: 1.415, g_loss: 0.968, rnn_loss: 0.010\n",
      "Epoch: [143/200] [  50/ 115] time: 0.5250s, d_loss: 1.336, g_loss: 1.498, rnn_loss: 0.013\n",
      "Epoch: [143/200] [ 100/ 115] time: 0.5211s, d_loss: 1.157, g_loss: 1.620, rnn_loss: 0.014\n",
      "Epoch: [144/200] [   0/ 115] time: 0.5266s, d_loss: 1.275, g_loss: 1.145, rnn_loss: 0.015\n",
      "Epoch: [144/200] [  50/ 115] time: 0.5230s, d_loss: 1.115, g_loss: 2.202, rnn_loss: 0.010\n",
      "Epoch: [144/200] [ 100/ 115] time: 0.5316s, d_loss: 1.184, g_loss: 1.457, rnn_loss: 0.005\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [145/200] [   0/ 115] time: 0.5357s, d_loss: 1.284, g_loss: 1.130, rnn_loss: 0.021\n",
      "Epoch: [145/200] [  50/ 115] time: 0.4634s, d_loss: 1.200, g_loss: 1.399, rnn_loss: 0.019\n",
      "Epoch: [145/200] [ 100/ 115] time: 0.4583s, d_loss: 1.185, g_loss: 1.664, rnn_loss: 0.020\n",
      "Epoch: [146/200] [   0/ 115] time: 0.4499s, d_loss: 1.479, g_loss: 1.577, rnn_loss: 0.007\n",
      "Epoch: [146/200] [  50/ 115] time: 0.4660s, d_loss: 1.139, g_loss: 2.261, rnn_loss: 0.010\n",
      "Epoch: [146/200] [ 100/ 115] time: 0.4754s, d_loss: 1.431, g_loss: 1.130, rnn_loss: 0.006\n",
      "Epoch: [147/200] [   0/ 115] time: 0.4608s, d_loss: 1.280, g_loss: 1.066, rnn_loss: 0.012\n",
      "Epoch: [147/200] [  50/ 115] time: 0.4887s, d_loss: 1.364, g_loss: 1.149, rnn_loss: 0.013\n",
      "Epoch: [147/200] [ 100/ 115] time: 0.4670s, d_loss: 1.136, g_loss: 1.703, rnn_loss: 0.012\n",
      "Epoch: [148/200] [   0/ 115] time: 0.4758s, d_loss: 1.274, g_loss: 1.275, rnn_loss: 0.004\n",
      "Epoch: [148/200] [  50/ 115] time: 0.4763s, d_loss: 1.182, g_loss: 1.809, rnn_loss: 0.008\n",
      "Epoch: [148/200] [ 100/ 115] time: 0.4769s, d_loss: 1.260, g_loss: 1.163, rnn_loss: 0.010\n",
      "Epoch: [149/200] [   0/ 115] time: 0.4804s, d_loss: 1.361, g_loss: 1.455, rnn_loss: 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [149/200] [  50/ 115] time: 0.4793s, d_loss: 1.198, g_loss: 1.519, rnn_loss: 0.001\n",
      "Epoch: [149/200] [ 100/ 115] time: 0.4647s, d_loss: 1.162, g_loss: 1.732, rnn_loss: 0.017\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [150/200] [   0/ 115] time: 0.4866s, d_loss: 1.287, g_loss: 1.304, rnn_loss: 0.014\n",
      "Epoch: [150/200] [  50/ 115] time: 0.4562s, d_loss: 1.247, g_loss: 1.351, rnn_loss: 0.000\n",
      "Epoch: [150/200] [ 100/ 115] time: 0.4428s, d_loss: 1.129, g_loss: 1.757, rnn_loss: 0.011\n",
      "Epoch: [151/200] [   0/ 115] time: 0.4548s, d_loss: 1.308, g_loss: 0.969, rnn_loss: 0.016\n",
      "Epoch: [151/200] [  50/ 115] time: 0.4472s, d_loss: 1.283, g_loss: 1.341, rnn_loss: 0.010\n",
      "Epoch: [151/200] [ 100/ 115] time: 0.5699s, d_loss: 1.311, g_loss: 1.438, rnn_loss: 0.012\n",
      "Epoch: [152/200] [   0/ 115] time: 0.5515s, d_loss: 1.250, g_loss: 1.479, rnn_loss: 0.022\n",
      "Epoch: [152/200] [  50/ 115] time: 0.5744s, d_loss: 1.266, g_loss: 1.315, rnn_loss: 0.006\n",
      "Epoch: [152/200] [ 100/ 115] time: 0.5381s, d_loss: 1.192, g_loss: 1.782, rnn_loss: 0.008\n",
      "Epoch: [153/200] [   0/ 115] time: 0.5507s, d_loss: 1.286, g_loss: 1.114, rnn_loss: 0.018\n",
      "Epoch: [153/200] [  50/ 115] time: 0.5251s, d_loss: 1.206, g_loss: 1.694, rnn_loss: 0.023\n",
      "Epoch: [153/200] [ 100/ 115] time: 0.5326s, d_loss: 1.259, g_loss: 1.380, rnn_loss: 0.010\n",
      "Epoch: [154/200] [   0/ 115] time: 0.5315s, d_loss: 1.131, g_loss: 1.934, rnn_loss: 0.011\n",
      "Epoch: [154/200] [  50/ 115] time: 0.5272s, d_loss: 1.197, g_loss: 1.937, rnn_loss: 0.019\n",
      "Epoch: [154/200] [ 100/ 115] time: 0.5196s, d_loss: 1.083, g_loss: 2.446, rnn_loss: 0.007\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [155/200] [   0/ 115] time: 0.5640s, d_loss: 1.217, g_loss: 1.036, rnn_loss: 0.017\n",
      "Epoch: [155/200] [  50/ 115] time: 0.5216s, d_loss: 1.292, g_loss: 1.392, rnn_loss: 0.005\n",
      "Epoch: [155/200] [ 100/ 115] time: 0.5176s, d_loss: 1.188, g_loss: 1.838, rnn_loss: 0.014\n",
      "Epoch: [156/200] [   0/ 115] time: 0.5245s, d_loss: 1.179, g_loss: 1.616, rnn_loss: 0.012\n",
      "Epoch: [156/200] [  50/ 115] time: 0.5247s, d_loss: 1.230, g_loss: 1.224, rnn_loss: 0.021\n",
      "Epoch: [156/200] [ 100/ 115] time: 0.5107s, d_loss: 1.300, g_loss: 1.386, rnn_loss: 0.015\n",
      "Epoch: [157/200] [   0/ 115] time: 0.5211s, d_loss: 1.274, g_loss: 1.473, rnn_loss: 0.029\n",
      "Epoch: [157/200] [  50/ 115] time: 0.5216s, d_loss: 1.278, g_loss: 1.310, rnn_loss: 0.010\n",
      "Epoch: [157/200] [ 100/ 115] time: 0.5203s, d_loss: 1.113, g_loss: 2.271, rnn_loss: 0.011\n",
      "Epoch: [158/200] [   0/ 115] time: 0.5246s, d_loss: 1.269, g_loss: 1.470, rnn_loss: 0.003\n",
      "Epoch: [158/200] [  50/ 115] time: 0.5216s, d_loss: 1.327, g_loss: 1.177, rnn_loss: 0.014\n",
      "Epoch: [158/200] [ 100/ 115] time: 0.5086s, d_loss: 1.220, g_loss: 1.405, rnn_loss: 0.006\n",
      "Epoch: [159/200] [   0/ 115] time: 0.5270s, d_loss: 1.332, g_loss: 1.287, rnn_loss: 0.024\n",
      "Epoch: [159/200] [  50/ 115] time: 0.5212s, d_loss: 1.312, g_loss: 1.248, rnn_loss: 0.004\n",
      "Epoch: [159/200] [ 100/ 115] time: 0.5246s, d_loss: 1.264, g_loss: 1.211, rnn_loss: 0.013\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [160/200] [   0/ 115] time: 0.5570s, d_loss: 1.228, g_loss: 1.449, rnn_loss: 0.013\n",
      "Epoch: [160/200] [  50/ 115] time: 0.5256s, d_loss: 1.367, g_loss: 1.197, rnn_loss: 0.004\n",
      "Epoch: [160/200] [ 100/ 115] time: 0.5271s, d_loss: 1.203, g_loss: 1.414, rnn_loss: 0.011\n",
      "Epoch: [161/200] [   0/ 115] time: 0.5381s, d_loss: 1.176, g_loss: 1.831, rnn_loss: 0.018\n",
      "Epoch: [161/200] [  50/ 115] time: 0.5163s, d_loss: 1.316, g_loss: 1.637, rnn_loss: 0.013\n",
      "Epoch: [161/200] [ 100/ 115] time: 0.5256s, d_loss: 1.139, g_loss: 2.105, rnn_loss: 0.003\n",
      "Epoch: [162/200] [   0/ 115] time: 0.5256s, d_loss: 1.322, g_loss: 1.582, rnn_loss: 0.011\n",
      "Epoch: [162/200] [  50/ 115] time: 0.5143s, d_loss: 1.404, g_loss: 0.877, rnn_loss: 0.009\n",
      "Epoch: [162/200] [ 100/ 115] time: 0.5359s, d_loss: 1.211, g_loss: 1.307, rnn_loss: 0.003\n",
      "Epoch: [163/200] [   0/ 115] time: 0.5331s, d_loss: 1.337, g_loss: 1.192, rnn_loss: 0.005\n",
      "Epoch: [163/200] [  50/ 115] time: 0.5455s, d_loss: 1.357, g_loss: 1.148, rnn_loss: 0.009\n",
      "Epoch: [163/200] [ 100/ 115] time: 0.5312s, d_loss: 1.218, g_loss: 1.722, rnn_loss: 0.014\n",
      "Epoch: [164/200] [   0/ 115] time: 0.5245s, d_loss: 1.213, g_loss: 1.914, rnn_loss: 0.011\n",
      "Epoch: [164/200] [  50/ 115] time: 0.5226s, d_loss: 1.332, g_loss: 1.286, rnn_loss: 0.012\n",
      "Epoch: [164/200] [ 100/ 115] time: 0.5219s, d_loss: 1.161, g_loss: 2.645, rnn_loss: 0.015\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [165/200] [   0/ 115] time: 0.5595s, d_loss: 1.159, g_loss: 2.161, rnn_loss: 0.009\n",
      "Epoch: [165/200] [  50/ 115] time: 0.5220s, d_loss: 1.293, g_loss: 1.068, rnn_loss: 0.011\n",
      "Epoch: [165/200] [ 100/ 115] time: 0.4533s, d_loss: 1.282, g_loss: 1.397, rnn_loss: 0.010\n",
      "Epoch: [166/200] [   0/ 115] time: 0.4673s, d_loss: 1.191, g_loss: 1.848, rnn_loss: 0.015\n",
      "Epoch: [166/200] [  50/ 115] time: 0.5342s, d_loss: 1.404, g_loss: 1.037, rnn_loss: 0.007\n",
      "Epoch: [166/200] [ 100/ 115] time: 0.5236s, d_loss: 1.287, g_loss: 1.856, rnn_loss: 0.015\n",
      "Epoch: [167/200] [   0/ 115] time: 0.5275s, d_loss: 1.278, g_loss: 1.973, rnn_loss: 0.006\n",
      "Epoch: [167/200] [  50/ 115] time: 0.5511s, d_loss: 1.215, g_loss: 1.832, rnn_loss: 0.006\n",
      "Epoch: [167/200] [ 100/ 115] time: 0.5336s, d_loss: 1.261, g_loss: 1.416, rnn_loss: 0.023\n",
      "Epoch: [168/200] [   0/ 115] time: 0.5356s, d_loss: 1.425, g_loss: 1.019, rnn_loss: 0.005\n",
      "Epoch: [168/200] [  50/ 115] time: 0.5420s, d_loss: 1.231, g_loss: 1.598, rnn_loss: 0.013\n",
      "Epoch: [168/200] [ 100/ 115] time: 0.5266s, d_loss: 1.243, g_loss: 1.310, rnn_loss: 0.002\n",
      "Epoch: [169/200] [   0/ 115] time: 0.5306s, d_loss: 1.366, g_loss: 1.175, rnn_loss: 0.012\n",
      "Epoch: [169/200] [  50/ 115] time: 0.5561s, d_loss: 1.435, g_loss: 0.970, rnn_loss: 0.017\n",
      "Epoch: [169/200] [ 100/ 115] time: 0.5281s, d_loss: 1.300, g_loss: 1.045, rnn_loss: 0.013\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [170/200] [   0/ 115] time: 0.5416s, d_loss: 1.249, g_loss: 1.268, rnn_loss: 0.013\n",
      "Epoch: [170/200] [  50/ 115] time: 0.5520s, d_loss: 1.163, g_loss: 1.565, rnn_loss: 0.004\n",
      "Epoch: [170/200] [ 100/ 115] time: 0.5266s, d_loss: 1.247, g_loss: 1.668, rnn_loss: 0.024\n",
      "Epoch: [171/200] [   0/ 115] time: 0.5169s, d_loss: 1.278, g_loss: 1.380, rnn_loss: 0.019\n",
      "Epoch: [171/200] [  50/ 115] time: 0.5305s, d_loss: 1.391, g_loss: 1.001, rnn_loss: 0.013\n",
      "Epoch: [171/200] [ 100/ 115] time: 0.5356s, d_loss: 1.178, g_loss: 2.664, rnn_loss: 0.007\n",
      "Epoch: [172/200] [   0/ 115] time: 0.5239s, d_loss: 1.387, g_loss: 1.125, rnn_loss: 0.019\n",
      "Epoch: [172/200] [  50/ 115] time: 0.5316s, d_loss: 1.190, g_loss: 1.855, rnn_loss: 0.006\n",
      "Epoch: [172/200] [ 100/ 115] time: 0.5198s, d_loss: 1.264, g_loss: 1.237, rnn_loss: 0.006\n",
      "Epoch: [173/200] [   0/ 115] time: 0.5226s, d_loss: 1.322, g_loss: 1.586, rnn_loss: 0.001\n",
      "Epoch: [173/200] [  50/ 115] time: 0.5426s, d_loss: 1.463, g_loss: 0.995, rnn_loss: 0.003\n",
      "Epoch: [173/200] [ 100/ 115] time: 0.5206s, d_loss: 1.228, g_loss: 1.629, rnn_loss: 0.018\n",
      "Epoch: [174/200] [   0/ 115] time: 0.5186s, d_loss: 1.210, g_loss: 1.945, rnn_loss: 0.010\n",
      "Epoch: [174/200] [  50/ 115] time: 0.5472s, d_loss: 1.444, g_loss: 0.830, rnn_loss: 0.012\n",
      "Epoch: [174/200] [ 100/ 115] time: 0.5336s, d_loss: 1.342, g_loss: 1.094, rnn_loss: 0.005\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [175/200] [   0/ 115] time: 0.5541s, d_loss: 1.266, g_loss: 1.794, rnn_loss: 0.017\n",
      "Epoch: [175/200] [  50/ 115] time: 0.5306s, d_loss: 1.360, g_loss: 1.045, rnn_loss: 0.007\n",
      "Epoch: [175/200] [ 100/ 115] time: 0.5409s, d_loss: 1.215, g_loss: 1.491, rnn_loss: 0.010\n",
      "Epoch: [176/200] [   0/ 115] time: 0.5242s, d_loss: 1.424, g_loss: 1.301, rnn_loss: 0.014\n",
      "Epoch: [176/200] [  50/ 115] time: 0.5345s, d_loss: 1.466, g_loss: 0.966, rnn_loss: 0.021\n",
      "Epoch: [176/200] [ 100/ 115] time: 0.5393s, d_loss: 1.367, g_loss: 0.705, rnn_loss: 0.011\n",
      "Epoch: [177/200] [   0/ 115] time: 0.5159s, d_loss: 1.437, g_loss: 1.159, rnn_loss: 0.018\n",
      "Epoch: [177/200] [  50/ 115] time: 0.5396s, d_loss: 1.310, g_loss: 1.312, rnn_loss: 0.022\n",
      "Epoch: [177/200] [ 100/ 115] time: 0.5392s, d_loss: 1.218, g_loss: 1.488, rnn_loss: 0.011\n",
      "Epoch: [178/200] [   0/ 115] time: 0.5196s, d_loss: 1.384, g_loss: 1.131, rnn_loss: 0.016\n",
      "Epoch: [178/200] [  50/ 115] time: 0.5353s, d_loss: 1.220, g_loss: 1.586, rnn_loss: 0.011\n",
      "Epoch: [178/200] [ 100/ 115] time: 0.5360s, d_loss: 1.255, g_loss: 1.198, rnn_loss: 0.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [179/200] [   0/ 115] time: 0.5213s, d_loss: 1.367, g_loss: 1.148, rnn_loss: 0.008\n",
      "Epoch: [179/200] [  50/ 115] time: 0.4506s, d_loss: 1.427, g_loss: 0.958, rnn_loss: 0.022\n",
      "Epoch: [179/200] [ 100/ 115] time: 0.5542s, d_loss: 1.314, g_loss: 1.072, rnn_loss: 0.021\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [180/200] [   0/ 115] time: 0.5837s, d_loss: 1.374, g_loss: 1.144, rnn_loss: 0.009\n",
      "Epoch: [180/200] [  50/ 115] time: 0.4648s, d_loss: 1.403, g_loss: 1.135, rnn_loss: 0.018\n",
      "Epoch: [180/200] [ 100/ 115] time: 0.4663s, d_loss: 1.274, g_loss: 1.738, rnn_loss: 0.018\n",
      "Epoch: [181/200] [   0/ 115] time: 0.4532s, d_loss: 1.333, g_loss: 1.368, rnn_loss: 0.017\n",
      "Epoch: [181/200] [  50/ 115] time: 0.4677s, d_loss: 1.233, g_loss: 1.716, rnn_loss: 0.010\n",
      "Epoch: [181/200] [ 100/ 115] time: 0.4683s, d_loss: 1.308, g_loss: 1.058, rnn_loss: 0.008\n",
      "Epoch: [182/200] [   0/ 115] time: 0.4561s, d_loss: 1.456, g_loss: 0.881, rnn_loss: 0.009\n",
      "Epoch: [182/200] [  50/ 115] time: 0.4573s, d_loss: 1.669, g_loss: 0.967, rnn_loss: 0.019\n",
      "Epoch: [182/200] [ 100/ 115] time: 0.4671s, d_loss: 1.268, g_loss: 1.520, rnn_loss: 0.017\n",
      "Epoch: [183/200] [   0/ 115] time: 0.4538s, d_loss: 1.468, g_loss: 0.959, rnn_loss: 0.005\n",
      "Epoch: [183/200] [  50/ 115] time: 0.4585s, d_loss: 1.580, g_loss: 1.064, rnn_loss: 0.014\n",
      "Epoch: [183/200] [ 100/ 115] time: 0.5230s, d_loss: 1.454, g_loss: 1.000, rnn_loss: 0.012\n",
      "Epoch: [184/200] [   0/ 115] time: 0.4798s, d_loss: 1.270, g_loss: 1.470, rnn_loss: 0.009\n",
      "Epoch: [184/200] [  50/ 115] time: 0.4868s, d_loss: 1.384, g_loss: 1.101, rnn_loss: 0.014\n",
      "Epoch: [184/200] [ 100/ 115] time: 0.4738s, d_loss: 1.374, g_loss: 1.009, rnn_loss: 0.012\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [185/200] [   0/ 115] time: 0.5117s, d_loss: 1.498, g_loss: 0.700, rnn_loss: 0.005\n",
      "Epoch: [185/200] [  50/ 115] time: 0.4891s, d_loss: 1.563, g_loss: 1.411, rnn_loss: 0.007\n",
      "Epoch: [185/200] [ 100/ 115] time: 0.4692s, d_loss: 1.344, g_loss: 1.236, rnn_loss: 0.027\n",
      "Epoch: [186/200] [   0/ 115] time: 0.4694s, d_loss: 1.372, g_loss: 1.054, rnn_loss: 0.009\n",
      "Epoch: [186/200] [  50/ 115] time: 0.4797s, d_loss: 1.404, g_loss: 1.085, rnn_loss: 0.015\n",
      "Epoch: [186/200] [ 100/ 115] time: 0.4677s, d_loss: 1.426, g_loss: 0.815, rnn_loss: 0.010\n",
      "Epoch: [187/200] [   0/ 115] time: 0.4653s, d_loss: 1.369, g_loss: 1.054, rnn_loss: 0.013\n",
      "Epoch: [187/200] [  50/ 115] time: 0.4662s, d_loss: 1.402, g_loss: 0.919, rnn_loss: 0.010\n",
      "Epoch: [187/200] [ 100/ 115] time: 0.4525s, d_loss: 1.423, g_loss: 1.336, rnn_loss: 0.006\n",
      "Epoch: [188/200] [   0/ 115] time: 0.4461s, d_loss: 1.321, g_loss: 1.262, rnn_loss: 0.014\n",
      "Epoch: [188/200] [  50/ 115] time: 0.4648s, d_loss: 1.428, g_loss: 1.107, rnn_loss: 0.005\n",
      "Epoch: [188/200] [ 100/ 115] time: 0.4604s, d_loss: 1.305, g_loss: 1.394, rnn_loss: 0.005\n",
      "Epoch: [189/200] [   0/ 115] time: 0.4418s, d_loss: 1.278, g_loss: 1.248, rnn_loss: 0.002\n",
      "Epoch: [189/200] [  50/ 115] time: 0.4677s, d_loss: 1.288, g_loss: 0.946, rnn_loss: 0.009\n",
      "Epoch: [189/200] [ 100/ 115] time: 0.4659s, d_loss: 1.406, g_loss: 0.994, rnn_loss: 0.012\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [190/200] [   0/ 115] time: 0.5623s, d_loss: 1.311, g_loss: 1.131, rnn_loss: 0.013\n",
      "Epoch: [190/200] [  50/ 115] time: 0.4718s, d_loss: 1.329, g_loss: 1.215, rnn_loss: 0.001\n",
      "Epoch: [190/200] [ 100/ 115] time: 0.5465s, d_loss: 1.376, g_loss: 0.972, rnn_loss: 0.011\n",
      "Epoch: [191/200] [   0/ 115] time: 0.5346s, d_loss: 1.247, g_loss: 1.472, rnn_loss: 0.010\n",
      "Epoch: [191/200] [  50/ 115] time: 0.5326s, d_loss: 1.502, g_loss: 0.904, rnn_loss: 0.016\n",
      "Epoch: [191/200] [ 100/ 115] time: 0.5262s, d_loss: 1.248, g_loss: 1.619, rnn_loss: 0.002\n",
      "Epoch: [192/200] [   0/ 115] time: 0.5159s, d_loss: 1.214, g_loss: 1.580, rnn_loss: 0.012\n",
      "Epoch: [192/200] [  50/ 115] time: 0.5291s, d_loss: 1.416, g_loss: 1.069, rnn_loss: 0.009\n",
      "Epoch: [192/200] [ 100/ 115] time: 0.5276s, d_loss: 1.261, g_loss: 1.385, rnn_loss: 0.009\n",
      "Epoch: [193/200] [   0/ 115] time: 0.5281s, d_loss: 1.328, g_loss: 1.313, rnn_loss: 0.026\n",
      "Epoch: [193/200] [  50/ 115] time: 0.5211s, d_loss: 1.220, g_loss: 1.363, rnn_loss: 0.012\n",
      "Epoch: [193/200] [ 100/ 115] time: 0.5286s, d_loss: 1.243, g_loss: 1.339, rnn_loss: 0.020\n",
      "Epoch: [194/200] [   0/ 115] time: 0.5204s, d_loss: 1.374, g_loss: 1.388, rnn_loss: 0.007\n",
      "Epoch: [194/200] [  50/ 115] time: 0.5356s, d_loss: 1.160, g_loss: 1.738, rnn_loss: 0.011\n",
      "Epoch: [194/200] [ 100/ 115] time: 0.5369s, d_loss: 1.401, g_loss: 1.039, rnn_loss: 0.010\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [195/200] [   0/ 115] time: 0.5501s, d_loss: 1.283, g_loss: 2.072, rnn_loss: 0.018\n",
      "Epoch: [195/200] [  50/ 115] time: 0.5191s, d_loss: 1.504, g_loss: 0.667, rnn_loss: 0.002\n",
      "Epoch: [195/200] [ 100/ 115] time: 0.5218s, d_loss: 1.361, g_loss: 0.917, rnn_loss: 0.002\n",
      "Epoch: [196/200] [   0/ 115] time: 0.5123s, d_loss: 1.460, g_loss: 1.264, rnn_loss: 0.019\n",
      "Epoch: [196/200] [  50/ 115] time: 0.5330s, d_loss: 1.166, g_loss: 1.735, rnn_loss: 0.008\n",
      "Epoch: [196/200] [ 100/ 115] time: 0.5221s, d_loss: 1.229, g_loss: 1.419, rnn_loss: 0.007\n",
      "Epoch: [197/200] [   0/ 115] time: 0.5276s, d_loss: 1.249, g_loss: 1.675, rnn_loss: 0.013\n",
      "Epoch: [197/200] [  50/ 115] time: 0.5341s, d_loss: 1.622, g_loss: 0.913, rnn_loss: 0.007\n",
      "Epoch: [197/200] [ 100/ 115] time: 0.5197s, d_loss: 1.457, g_loss: 0.866, rnn_loss: 0.008\n",
      "Epoch: [198/200] [   0/ 115] time: 0.5140s, d_loss: 1.204, g_loss: 1.746, rnn_loss: 0.019\n",
      "Epoch: [198/200] [  50/ 115] time: 0.5321s, d_loss: 1.270, g_loss: 0.892, rnn_loss: 0.013\n",
      "Epoch: [198/200] [ 100/ 115] time: 0.5226s, d_loss: 1.616, g_loss: 0.817, rnn_loss: 0.017\n",
      "Epoch: [199/200] [   0/ 115] time: 0.5226s, d_loss: 1.216, g_loss: 2.026, rnn_loss: 0.004\n",
      "Epoch: [199/200] [  50/ 115] time: 0.5366s, d_loss: 1.336, g_loss: 1.240, rnn_loss: 0.005\n",
      "Epoch: [199/200] [ 100/ 115] time: 0.5283s, d_loss: 1.419, g_loss: 1.028, rnn_loss: 0.018\n",
      "-----success saved checkpoint--------\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(get_hparas(), training_phase=True, dataset_path=data_path, ckpt_path=checkpoint_path, inference_path=inference_path)\n",
    "gan.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator_test(filenames, batch_size):\n",
    "    data = pd.read_pickle(filenames)\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "iterator_train, types, shapes = data_iterator_test(data_path+'/testData.pkl', 64)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    caption, idex = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/rnn_model_199.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/g_model_199.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/ca_model_199.ckpt\n",
      "-----success restored checkpoint--------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "gan = GAN(get_hparas(), training_phase=False, dataset_path=data_path, ckpt_path=checkpoint_path, inference_path=inference_path, recover=199)\n",
    "img = gan.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "iterator_train, types, shapes = data_iterator(data_path+'/text2ImgData.pkl', BATCH_SIZE, training_data_generator)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    image, text = sess.run(next_element)\n",
    "print(text.shape)\n",
    "img = gan.test_pred(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cd testing && python ./inception_score.py ../inference ../socre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
